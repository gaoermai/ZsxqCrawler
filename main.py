"""
çŸ¥è¯†æ˜Ÿçƒæ•°æ®é‡‡é›†å™¨ - FastAPI åç«¯æœåŠ¡
æä¾›RESTful APIæ¥å£æ¥æ“ä½œç°æœ‰çš„çˆ¬è™«åŠŸèƒ½
"""

import os
import sys
import asyncio
from typing import Dict, Any, Optional, List
from datetime import datetime
import json
import requests

from fastapi import FastAPI, HTTPException, BackgroundTasks
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import StreamingResponse, Response
from pydantic import BaseModel, Field
import uvicorn
import mimetypes
import random
import time

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„ï¼ˆç°åœ¨main.pyå°±åœ¨æ ¹ç›®å½•ï¼‰
project_root = os.path.dirname(os.path.abspath(__file__))
if project_root not in sys.path:
    sys.path.append(project_root)

# å¯¼å…¥ç°æœ‰çš„ä¸šåŠ¡é€»è¾‘æ¨¡å—
from zsxq_interactive_crawler import ZSXQInteractiveCrawler, load_config
from db_path_manager import get_db_path_manager
from image_cache_manager import get_image_cache_manager
from accounts_manager import (
    get_accounts as am_get_accounts,
    add_account as am_add_account,
    delete_account as am_delete_account,
    set_default_account as am_set_default_account,
    assign_group_account as am_assign_group_account,
    get_account_for_group as am_get_account_for_group,
    get_account_summary_for_group as am_get_account_summary_for_group,
    get_default_account as am_get_default_account,
    get_account_by_id as am_get_account_by_id,
)
from account_info_db import get_account_info_db

app = FastAPI(
    title="çŸ¥è¯†æ˜Ÿçƒæ•°æ®é‡‡é›†å™¨ API",
    description="ä¸ºçŸ¥è¯†æ˜Ÿçƒæ•°æ®é‡‡é›†å™¨æä¾›RESTful APIæ¥å£",
    version="1.0.0"
)

# é…ç½®CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # å‰ç«¯åœ°å€
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# å…¨å±€å˜é‡å­˜å‚¨çˆ¬è™«å®ä¾‹å’Œä»»åŠ¡çŠ¶æ€
crawler_instance: Optional[ZSXQInteractiveCrawler] = None
current_tasks: Dict[str, Dict[str, Any]] = {}
task_counter = 0
task_logs: Dict[str, List[str]] = {}  # å­˜å‚¨ä»»åŠ¡æ—¥å¿—
sse_connections: Dict[str, List] = {}  # å­˜å‚¨SSEè¿æ¥
task_stop_flags: Dict[str, bool] = {}  # ä»»åŠ¡åœæ­¢æ ‡å¿—
file_downloader_instances: Dict[str, Any] = {}  # å­˜å‚¨æ–‡ä»¶ä¸‹è½½å™¨å®ä¾‹

# =========================
# æœ¬åœ°ç¾¤æ‰«æï¼ˆoutput ç›®å½•ï¼‰
# =========================

# å¯é…ç½®ï¼šé»˜è®¤ ./outputï¼›å¯é€šè¿‡ç¯å¢ƒå˜é‡ OUTPUT_DIR è¦†ç›–
LOCAL_OUTPUT_DIR = os.environ.get("OUTPUT_DIR", "output")
# å¤„ç†ä¸Šé™ä¿æŠ¤ï¼Œé»˜è®¤ 10000ï¼›å¯é€šè¿‡ LOCAL_GROUPS_SCAN_LIMIT è¦†ç›–
try:
    LOCAL_SCAN_LIMIT = int(os.environ.get("LOCAL_GROUPS_SCAN_LIMIT", "10000"))
except Exception:
    LOCAL_SCAN_LIMIT = 10000

# æœ¬åœ°ç¾¤ç¼“å­˜
_local_groups_cache = {
    "ids": set(),     # set[int]
    "scanned_at": 0.0 # epoch ç§’
}


def _safe_listdir(path: str):
    """å®‰å…¨åˆ—ç›®å½•ï¼Œå¼‚å¸¸ä¸æŠ›å‡ºï¼Œè¿”å›ç©ºåˆ—è¡¨å¹¶å‘Šè­¦"""
    try:
        return os.listdir(path)
    except Exception as e:
        print(f"âš ï¸ æ— æ³•è¯»å–ç›®å½• {path}: {e}")
        return []


def _collect_numeric_dirs(base: str, limit: int) -> set:
    """
    æ‰«æ base çš„ä¸€çº§å­ç›®å½•ï¼Œæ”¶é›†çº¯æ•°å­—ç›®å½•åï¼ˆ^\d+$ï¼‰ä½œä¸ºç¾¤IDã€‚
    å¿½ç•¥ï¼šéç›®å½•ã€è½¯é“¾æ¥ã€éšè—ç›®å½•ï¼ˆä»¥ . å¼€å¤´ï¼‰ã€‚
    """
    ids = set()
    if not base:
        return ids

    base_abs = os.path.abspath(base)
    if not (os.path.exists(base_abs) and os.path.isdir(base_abs)):
        # è§†ä¸ºç©ºé›†åˆï¼Œä¸æŠ¥é”™
        print(f"âš ï¸ ç›®å½•ä¸å­˜åœ¨æˆ–ä¸å¯è¯»: {base_abs}ï¼Œè§†ä¸ºç©ºé›†åˆ")
        return ids

    processed = 0
    for name in _safe_listdir(base_abs):
        # éšè—ç›®å½•
        if not name or name.startswith('.'):
            continue

        path = os.path.join(base_abs, name)
        try:
            # è½¯é“¾æ¥/éç›®å½•å¿½ç•¥
            if os.path.islink(path) or not os.path.isdir(path):
                continue

            # ä»…çº¯æ•°å­—ç›®å½•å
            if name.isdigit():
                ids.add(int(name))
                processed += 1
                if processed >= limit:
                    print(f"âš ï¸ å­ç›®å½•æ•°é‡è¶…è¿‡ä¸Šé™ {limit}ï¼Œå·²æˆªæ–­")
                    break
        except Exception:
            # å•é¡¹å¤±è´¥å®‰å…¨é™çº§
            continue

    return ids


def scan_local_groups(output_dir: str = None, limit: int = None) -> set:
    """
    æ‰«ææœ¬åœ° output çš„ä¸€çº§å­ç›®å½•ï¼Œè·å–ç¾¤IDé›†åˆã€‚
    åŒæ—¶å…¼å®¹ output/databases ç»“æ„ï¼ˆå¦‚å­˜åœ¨ï¼‰ã€‚
    åŒæ­¥æ‰§è¡Œï¼ˆç”¨äºæ‰‹åŠ¨åˆ·æ–°æˆ–å¼ºåˆ¶åˆ·æ–°ï¼‰ï¼Œå¼‚å¸¸å®‰å…¨é™çº§ã€‚
    """
    try:
        odir = output_dir or LOCAL_OUTPUT_DIR
        lim = int(limit or LOCAL_SCAN_LIMIT)

        # ä¸»è·¯å¾„ï¼šä»…æ‰«æ output çš„ä¸€çº§å­ç›®å½•
        ids_primary = _collect_numeric_dirs(odir, lim)

        # å…¼å®¹è·¯å¾„ï¼šoutput/databases çš„ä¸€çº§å­ç›®å½•ï¼ˆè‹¥å­˜åœ¨ï¼‰
        ids_secondary = _collect_numeric_dirs(os.path.join(odir, "databases"), lim)

        ids = set(ids_primary) | set(ids_secondary)

        # æ›´æ–°ç¼“å­˜
        _local_groups_cache["ids"] = ids
        _local_groups_cache["scanned_at"] = time.time()

        return ids
    except Exception as e:
        print(f"âš ï¸ æœ¬åœ°ç¾¤æ‰«æå¼‚å¸¸: {e}")
        # å®‰å…¨é™çº§ä¸ºæ—§ç¼“å­˜
        return _local_groups_cache.get("ids", set())


def get_cached_local_group_ids(force_refresh: bool = False) -> set:
    """
    è·å–ç¼“å­˜ä¸­çš„æœ¬åœ°ç¾¤IDé›†åˆï¼›å¯é€‰å¼ºåˆ¶åˆ·æ–°ã€‚
    æœªæ‰«æè¿‡æˆ–è¦æ±‚å¼ºæ›´æ—¶è§¦å‘åŒæ­¥æ‰«æã€‚
    """
    if force_refresh or not _local_groups_cache.get("ids"):
        return scan_local_groups()
    return _local_groups_cache.get("ids", set())


@app.on_event("startup")
async def _init_local_groups_scan():
    """
    åº”ç”¨å¯åŠ¨æ—¶åå°å¼‚æ­¥æ‰§è¡Œä¸€æ¬¡æ‰«æï¼Œä¸é˜»å¡ä¸»çº¿ç¨‹ã€‚
    """
    try:
        await asyncio.to_thread(scan_local_groups)
    except Exception as e:
        print(f"âš ï¸ å¯åŠ¨æ‰«ææœ¬åœ°ç¾¤å¤±è´¥: {e}")
# Pydanticæ¨¡å‹å®šä¹‰
class ConfigModel(BaseModel):
    cookie: str = Field(..., description="çŸ¥è¯†æ˜ŸçƒCookie")
    group_id: str = Field(..., description="ç¾¤ç»„ID")
    db_path: str = Field(default="zsxq_interactive.db", description="æ•°æ®åº“è·¯å¾„")

class CrawlHistoricalRequest(BaseModel):
    pages: int = Field(default=10, ge=1, le=1000, description="çˆ¬å–é¡µæ•°")
    per_page: int = Field(default=20, ge=1, le=100, description="æ¯é¡µæ•°é‡")
    crawlIntervalMin: Optional[float] = Field(default=None, ge=1.0, le=60.0, description="çˆ¬å–é—´éš”æœ€å°å€¼(ç§’)")
    crawlIntervalMax: Optional[float] = Field(default=None, ge=1.0, le=60.0, description="çˆ¬å–é—´éš”æœ€å¤§å€¼(ç§’)")
    longSleepIntervalMin: Optional[float] = Field(default=None, ge=60.0, le=3600.0, description="é•¿ä¼‘çœ é—´éš”æœ€å°å€¼(ç§’)")
    longSleepIntervalMax: Optional[float] = Field(default=None, ge=60.0, le=3600.0, description="é•¿ä¼‘çœ é—´éš”æœ€å¤§å€¼(ç§’)")
    pagesPerBatch: Optional[int] = Field(default=None, ge=5, le=50, description="æ¯æ‰¹æ¬¡é¡µé¢æ•°")

class CrawlSettingsRequest(BaseModel):
    crawlIntervalMin: Optional[float] = Field(default=None, ge=1.0, le=60.0, description="çˆ¬å–é—´éš”æœ€å°å€¼(ç§’)")
    crawlIntervalMax: Optional[float] = Field(default=None, ge=1.0, le=60.0, description="çˆ¬å–é—´éš”æœ€å¤§å€¼(ç§’)")
    longSleepIntervalMin: Optional[float] = Field(default=None, ge=60.0, le=3600.0, description="é•¿ä¼‘çœ é—´éš”æœ€å°å€¼(ç§’)")
    longSleepIntervalMax: Optional[float] = Field(default=None, ge=60.0, le=3600.0, description="é•¿ä¼‘çœ é—´éš”æœ€å¤§å€¼(ç§’)")
    pagesPerBatch: Optional[int] = Field(default=None, ge=5, le=50, description="æ¯æ‰¹æ¬¡é¡µé¢æ•°")

class FileDownloadRequest(BaseModel):
    max_files: Optional[int] = Field(default=None, description="æœ€å¤§ä¸‹è½½æ–‡ä»¶æ•°")
    sort_by: str = Field(default="download_count", description="æ’åºæ–¹å¼: download_count æˆ– time")
    download_interval: float = Field(default=1.0, ge=0.1, le=300.0, description="å•æ¬¡ä¸‹è½½é—´éš”ï¼ˆç§’ï¼‰")
    long_sleep_interval: float = Field(default=60.0, ge=10.0, le=3600.0, description="é•¿ä¼‘çœ é—´éš”ï¼ˆç§’ï¼‰")
    files_per_batch: int = Field(default=10, ge=1, le=100, description="ä¸‹è½½å¤šå°‘æ–‡ä»¶åè§¦å‘é•¿ä¼‘çœ ")
    # éšæœºé—´éš”èŒƒå›´å‚æ•°ï¼ˆå¯é€‰ï¼‰
    download_interval_min: Optional[float] = Field(default=None, ge=1.0, le=300.0, description="éšæœºä¸‹è½½é—´éš”æœ€å°å€¼ï¼ˆç§’ï¼‰")
    download_interval_max: Optional[float] = Field(default=None, ge=1.0, le=300.0, description="éšæœºä¸‹è½½é—´éš”æœ€å¤§å€¼ï¼ˆç§’ï¼‰")
    long_sleep_interval_min: Optional[float] = Field(default=None, ge=10.0, le=3600.0, description="éšæœºé•¿ä¼‘çœ é—´éš”æœ€å°å€¼ï¼ˆç§’ï¼‰")
    long_sleep_interval_max: Optional[float] = Field(default=None, ge=10.0, le=3600.0, description="éšæœºé•¿ä¼‘çœ é—´éš”æœ€å¤§å€¼ï¼ˆç§’ï¼‰")

class AccountCreateRequest(BaseModel):
    cookie: str = Field(..., description="è´¦å·Cookie")
    name: Optional[str] = Field(default=None, description="è´¦å·åç§°")
    make_default: Optional[bool] = Field(default=False, description="æ˜¯å¦è®¾ä¸ºé»˜è®¤è´¦å·")

class AssignGroupAccountRequest(BaseModel):
    account_id: str = Field(..., description="è´¦å·ID")

class GroupInfo(BaseModel):
    group_id: int
    name: str
    type: str
    background_url: Optional[str] = None
    owner: Optional[dict] = None
    statistics: Optional[dict] = None

class TaskResponse(BaseModel):
    task_id: str
    status: str  # pending, running, completed, failed
    message: str
    result: Optional[Dict[str, Any]] = None
    created_at: datetime
    updated_at: datetime

# è¾…åŠ©å‡½æ•°
def get_crawler(log_callback=None) -> ZSXQInteractiveCrawler:
    """è·å–çˆ¬è™«å®ä¾‹"""
    global crawler_instance
    if crawler_instance is None:
        config = load_config()
        if not config:
            raise HTTPException(status_code=500, detail="é…ç½®æ–‡ä»¶åŠ è½½å¤±è´¥")

        auth_config = config.get('auth', {})

        cookie = auth_config.get('cookie', '')
        group_id = auth_config.get('group_id', '')

        if cookie == "your_cookie_here" or group_id == "your_group_id_here" or not cookie or not group_id:
            raise HTTPException(status_code=400, detail="è¯·å…ˆåœ¨config.tomlä¸­é…ç½®Cookieå’Œç¾¤ç»„ID")

        # ä½¿ç”¨è·¯å¾„ç®¡ç†å™¨è·å–æ•°æ®åº“è·¯å¾„
        path_manager = get_db_path_manager()
        db_path = path_manager.get_topics_db_path(group_id)

        crawler_instance = ZSXQInteractiveCrawler(cookie, group_id, db_path, log_callback)

    return crawler_instance

def get_crawler_for_group(group_id: str, log_callback=None) -> ZSXQInteractiveCrawler:
    """ä¸ºæŒ‡å®šç¾¤ç»„è·å–çˆ¬è™«å®ä¾‹"""
    config = load_config()
    if not config:
        raise HTTPException(status_code=500, detail="é…ç½®æ–‡ä»¶åŠ è½½å¤±è´¥")

    # è‡ªåŠ¨åŒ¹é…è¯¥ç¾¤ç»„æ‰€å±è´¦å·ï¼Œè·å–å¯¹åº”Cookie
    cookie = get_cookie_for_group(group_id)

    if not cookie or cookie == "your_cookie_here":
        raise HTTPException(status_code=400, detail="æœªæ‰¾åˆ°å¯ç”¨Cookieï¼Œè¯·å…ˆåœ¨è´¦å·ç®¡ç†æˆ–config.tomlä¸­é…ç½®")

    # ä½¿ç”¨è·¯å¾„ç®¡ç†å™¨è·å–æŒ‡å®šç¾¤ç»„çš„æ•°æ®åº“è·¯å¾„
    path_manager = get_db_path_manager()
    db_path = path_manager.get_topics_db_path(group_id)

    return ZSXQInteractiveCrawler(cookie, group_id, db_path, log_callback)

def get_crawler_safe() -> Optional[ZSXQInteractiveCrawler]:
    """å®‰å…¨è·å–çˆ¬è™«å®ä¾‹ï¼Œé…ç½®æœªè®¾ç½®æ—¶è¿”å›None"""
    try:
        return get_crawler()
    except HTTPException:
        return None

def is_configured() -> bool:
    """æ£€æŸ¥æ˜¯å¦å·²é…ç½®è®¤è¯ä¿¡æ¯"""
    try:
        config = load_config()
        if not config:
            return False

        auth_config = config.get('auth', {})
        cookie = auth_config.get('cookie', '')
        group_id = auth_config.get('group_id', '')

        return (cookie != "your_cookie_here" and
                group_id != "your_group_id_here" and
                cookie and group_id)
    except:
        return False

def create_task(task_type: str, description: str) -> str:
    """åˆ›å»ºæ–°ä»»åŠ¡"""
    global task_counter
    task_counter += 1
    task_id = f"task_{task_counter}_{int(datetime.now().timestamp())}"
    
    current_tasks[task_id] = {
        "task_id": task_id,
        "type": task_type,
        "status": "pending",
        "message": description,
        "result": None,
        "created_at": datetime.now(),
        "updated_at": datetime.now()
    }

    # åˆå§‹åŒ–ä»»åŠ¡æ—¥å¿—å’Œåœæ­¢æ ‡å¿—
    task_logs[task_id] = []
    task_stop_flags[task_id] = False
    add_task_log(task_id, f"ä»»åŠ¡åˆ›å»º: {description}")

    return task_id

def add_task_log(task_id: str, log_message: str):
    """æ·»åŠ ä»»åŠ¡æ—¥å¿—"""
    if task_id not in task_logs:
        task_logs[task_id] = []

    timestamp = datetime.now().strftime("%H:%M:%S")
    formatted_log = f"[{timestamp}] {log_message}"
    task_logs[task_id].append(formatted_log)

    # å¹¿æ’­æ—¥å¿—åˆ°æ‰€æœ‰SSEè¿æ¥
    broadcast_log(task_id, formatted_log)

def broadcast_log(task_id: str, log_message: str):
    """å¹¿æ’­æ—¥å¿—åˆ°SSEè¿æ¥"""
    # è¿™ä¸ªå‡½æ•°ç°åœ¨ä¸»è¦ç”¨äºå­˜å‚¨æ—¥å¿—ï¼Œå®é™…çš„SSEå¹¿æ’­åœ¨streamç«¯ç‚¹ä¸­å®ç°
    pass

def build_stealth_headers(cookie: str) -> Dict[str, str]:
    """æ„é€ æ›´æ¥è¿‘å®˜ç½‘çš„è¯·æ±‚å¤´ï¼Œæå‡æˆåŠŸç‡"""
    user_agents = [
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36",
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/130.0.0.0 Safari/537.36",
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:132.0) Gecko/20100101 Firefox/132.0",
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36 Edg/131.0.0.0",
        "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36",
        "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36",
    ]
    headers = {
        "Accept": "application/json, text/plain, */*",
        "Accept-Encoding": "gzip, deflate, br, zstd",
        "Accept-Language": "zh-CN,zh;q=0.9,en;q=0.8,zh-TW;q=0.7",
        "Cache-Control": "no-cache",
        "Cookie": cookie,
        "Origin": "https://wx.zsxq.com",
        "Pragma": "no-cache",
        "Priority": "u=1, i",
        "Referer": "https://wx.zsxq.com/",
        "Sec-Ch-Ua": "\"Google Chrome\";v=\"137\", \"Chromium\";v=\"137\", \"Not/A)Brand\";v=\"24\"",
        "Sec-Ch-Ua-Mobile": "?0",
        "Sec-Ch-Ua-Platform": "\"Windows\"",
        "Sec-Fetch-Dest": "empty",
        "Sec-Fetch-Mode": "cors",
        "Sec-Fetch-Site": "same-site",
        "User-Agent": random.choice(user_agents),
        "X-Aduid": "a3be07cd6-dd67-3912-0093-862d844e7fe",
        "X-Request-Id": f"dcc5cb6ab-1bc3-8273-cc26-{random.randint(100000000000, 999999999999)}",
        "X-Signature": "733fd672ddf6d4e367730d9622cdd1e28a4b6203",
        "X-Timestamp": str(int(time.time())),
        "X-Version": "2.77.0",
    }
    return headers

def update_task(task_id: str, status: str, message: str, result: Optional[Dict[str, Any]] = None):
    """æ›´æ–°ä»»åŠ¡çŠ¶æ€"""
    if task_id in current_tasks:
        current_tasks[task_id].update({
            "status": status,
            "message": message,
            "result": result,
            "updated_at": datetime.now()
        })

        # æ·»åŠ çŠ¶æ€å˜æ›´æ—¥å¿—
        add_task_log(task_id, f"çŠ¶æ€æ›´æ–°: {message}")

def stop_task(task_id: str) -> bool:
    """åœæ­¢ä»»åŠ¡"""
    if task_id not in current_tasks:
        return False

    task = current_tasks[task_id]

    if task["status"] not in ["pending", "running"]:
        return False

    # è®¾ç½®åœæ­¢æ ‡å¿—
    task_stop_flags[task_id] = True
    add_task_log(task_id, "ğŸ›‘ æ”¶åˆ°åœæ­¢è¯·æ±‚ï¼Œæ­£åœ¨åœæ­¢ä»»åŠ¡...")

    # å¦‚æœæœ‰çˆ¬è™«å®ä¾‹ï¼Œä¹Ÿè®¾ç½®çˆ¬è™«çš„åœæ­¢æ ‡å¿—
    global crawler_instance, file_downloader_instances
    if crawler_instance:
        crawler_instance.set_stop_flag()

    # å¦‚æœæœ‰æ–‡ä»¶ä¸‹è½½å™¨å®ä¾‹ï¼Œä¹Ÿè®¾ç½®åœæ­¢æ ‡å¿—
    if task_id in file_downloader_instances:
        downloader = file_downloader_instances[task_id]
        downloader.set_stop_flag()

    update_task(task_id, "cancelled", "ä»»åŠ¡å·²è¢«ç”¨æˆ·åœæ­¢")

    return True

def is_task_stopped(task_id: str) -> bool:
    """æ£€æŸ¥ä»»åŠ¡æ˜¯å¦è¢«åœæ­¢"""
    stopped = task_stop_flags.get(task_id, False)
    return stopped

# APIè·¯ç”±å®šä¹‰
@app.get("/")
async def root():
    """æ ¹è·¯å¾„"""
    return {"message": "çŸ¥è¯†æ˜Ÿçƒæ•°æ®é‡‡é›†å™¨ API æœåŠ¡", "version": "1.0.0"}

@app.get("/api/health")
async def health_check():
    """å¥åº·æ£€æŸ¥"""
    return {"status": "healthy", "timestamp": datetime.now()}

@app.get("/api/config")
async def get_config():
    """è·å–å½“å‰é…ç½®"""
    try:
        config = load_config()
        if not config:
            raise HTTPException(status_code=500, detail="é…ç½®æ–‡ä»¶ä¸å­˜åœ¨")

        auth_config = config.get('auth', {})
        cookie = auth_config.get('cookie', '')
        group_id = auth_config.get('group_id', '')

        # æ£€æŸ¥é…ç½®çŠ¶æ€
        configured = is_configured()

        # éšè—æ•æ„Ÿä¿¡æ¯
        safe_config = {
            "configured": configured,
            "auth": {
                "cookie": "***" if configured else "æœªé…ç½®",
                "group_id": group_id if group_id != "your_group_id_here" else "æœªé…ç½®"
            },
            "database": config.get('database', {}),
            "download": config.get('download', {})
        }

        return safe_config
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"è·å–é…ç½®å¤±è´¥: {str(e)}")

@app.post("/api/config")
async def update_config(config: ConfigModel):
    """æ›´æ–°é…ç½®"""
    try:
        # ä½¿ç”¨è·¯å¾„ç®¡ç†å™¨è·å–æ•°æ®åº“è·¯å¾„
        path_manager = get_db_path_manager()
        topics_db_path = path_manager.get_topics_db_path(config.group_id)
        from pathlib import Path
        safe_topics_db_path = Path(topics_db_path).as_posix()

        # åˆ›å»ºé…ç½®å†…å®¹
        config_content = f"""# çŸ¥è¯†æ˜Ÿçƒæ•°æ®é‡‡é›†å™¨é…ç½®æ–‡ä»¶
# é€šè¿‡Webç•Œé¢è‡ªåŠ¨ç”Ÿæˆ

[auth]
# çŸ¥è¯†æ˜Ÿçƒç™»å½•Cookie
cookie = "{config.cookie}"

# çŸ¥è¯†æ˜Ÿçƒç¾¤ç»„ID
group_id = "{config.group_id}"

[database]
# æ•°æ®åº“æ–‡ä»¶è·¯å¾„ï¼ˆç”±è·¯å¾„ç®¡ç†å™¨è‡ªåŠ¨ç®¡ç†ï¼‰
path = "{safe_topics_db_path}"

[download]
# ä¸‹è½½ç›®å½•
dir = "downloads"
"""

        # ä¿å­˜é…ç½®æ–‡ä»¶
        config_path = "config.toml"
        with open(config_path, 'w', encoding='utf-8') as f:
            f.write(config_content)

        # é‡ç½®çˆ¬è™«å®ä¾‹ï¼Œå¼ºåˆ¶é‡æ–°åŠ è½½é…ç½®
        global crawler_instance
        crawler_instance = None

        return {"message": "é…ç½®æ›´æ–°æˆåŠŸ", "success": True}
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"æ›´æ–°é…ç½®å¤±è´¥: {str(e)}")

# è´¦å·ç®¡ç† API
@app.get("/api/accounts")
async def list_accounts():
    try:
        accounts = am_get_accounts(mask_cookie=True)
        # è‹¥æœªæ·»åŠ æœ¬åœ°è´¦å·ï¼Œä½†åœ¨ config.toml ä¸­å­˜åœ¨é»˜è®¤ Cookieï¼Œåˆ™è¿”å›ä¸€ä¸ªâ€œé»˜è®¤è´¦å·â€å ä½
        if not accounts:
            cfg = load_config()
            auth = cfg.get('auth', {}) if cfg else {}
            default_cookie = auth.get('cookie', '')
            if default_cookie and default_cookie != "your_cookie_here":
                accounts = [{
                    "id": "default",
                    "name": "é»˜è®¤è´¦å·",
                    "cookie": "***",
                    "is_default": True,
                    "created_at": None,
                }]
        return {"accounts": accounts}
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"è·å–è´¦å·åˆ—è¡¨å¤±è´¥: {str(e)}")

@app.post("/api/accounts")
async def create_account(request: AccountCreateRequest):
    try:
        acc = am_add_account(request.cookie, request.name, request.make_default or False)
        safe_acc = am_get_account_by_id(acc.get("id"), mask_cookie=True)
        return {"account": safe_acc}
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"æ–°å¢è´¦å·å¤±è´¥: {str(e)}")

@app.delete("/api/accounts/{account_id}")
async def remove_account(account_id: str):
    try:
        ok = delete_account_success = am_delete_account(account_id)
        if not ok:
            raise HTTPException(status_code=404, detail="è´¦å·ä¸å­˜åœ¨")
        return {"success": True}
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"åˆ é™¤è´¦å·å¤±è´¥: {str(e)}")

@app.post("/api/accounts/{account_id}/default")
async def make_default_account(account_id: str):
    try:
        ok = am_set_default_account(account_id)
        if not ok:
            raise HTTPException(status_code=404, detail="è´¦å·ä¸å­˜åœ¨")
        return {"success": True}
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"è®¾ç½®é»˜è®¤è´¦å·å¤±è´¥: {str(e)}")

@app.post("/api/groups/{group_id}/assign-account")
async def assign_account_to_group(group_id: str, request: AssignGroupAccountRequest):
    try:
        ok, msg = am_assign_group_account(group_id, request.account_id)
        if not ok:
            raise HTTPException(status_code=400, detail=msg)
        return {"success": True, "message": msg}
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"åˆ†é…è´¦å·å¤±è´¥: {str(e)}")

@app.get("/api/groups/{group_id}/account")
async def get_group_account(group_id: str):
    try:
        summary = get_account_summary_for_group_auto(group_id)
        return {"account": summary}
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"è·å–ç¾¤ç»„è´¦å·å¤±è´¥: {str(e)}")

# è´¦å·â€œè‡ªæˆ‘ä¿¡æ¯â€æŒä¹…åŒ– (/v3/users/self)
@app.get("/api/accounts/{account_id}/self")
async def get_account_self(account_id: str):
    """è·å–å¹¶è¿”å›æŒ‡å®šè´¦å·çš„å·²æŒä¹…åŒ–è‡ªæˆ‘ä¿¡æ¯ï¼›è‹¥æ— åˆ™å°è¯•æŠ“å–å¹¶ä¿å­˜"""
    try:
        db = get_account_info_db()
        info = db.get_self_info(account_id)
        if info:
            return {"self": info}

        # è‹¥æ•°æ®åº“æ— è®°å½•åˆ™æŠ“å–ï¼Œæ”¯æŒ 'default' ä¼ªè´¦å·ï¼ˆæ¥è‡ª config.tomlï¼‰
        cookie = None
        acc = am_get_account_by_id(account_id, mask_cookie=False)
        if acc:
            cookie = acc.get("cookie", "")
        elif account_id == "default":
            cfg = load_config()
            auth = cfg.get('auth', {}) if cfg else {}
            cookie = auth.get('cookie', '')
        else:
            raise HTTPException(status_code=404, detail="è´¦å·ä¸å­˜åœ¨")

        if not cookie:
            raise HTTPException(status_code=400, detail="è´¦å·æœªé…ç½®Cookie")

        headers = build_stealth_headers(cookie)
        resp = requests.get('https://api.zsxq.com/v3/users/self', headers=headers, timeout=30)
        resp.raise_for_status()
        data = resp.json()
        if not data.get('succeeded'):
            raise HTTPException(status_code=400, detail="APIè¿”å›å¤±è´¥")

        rd = data.get('resp_data', {}) or {}
        user = rd.get('user', {}) or {}
        wechat = (rd.get('accounts', {}) or {}).get('wechat', {}) or {}

        self_info = {
            "uid": user.get("uid"),
            "name": user.get("name") or wechat.get("name"),
            "avatar_url": user.get("avatar_url") or wechat.get("avatar_url"),
            "location": user.get("location"),
            "user_sid": user.get("user_sid"),
            "grade": user.get("grade"),
        }
        db.upsert_self_info(account_id, self_info, raw_json=data)
        return {"self": db.get_self_info(account_id)}
    except HTTPException:
        raise
    except requests.RequestException as e:
        raise HTTPException(status_code=502, detail=f"ç½‘ç»œè¯·æ±‚å¤±è´¥: {str(e)}")
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"è·å–è´¦å·ä¿¡æ¯å¤±è´¥: {str(e)}")

@app.post("/api/accounts/{account_id}/self/refresh")
async def refresh_account_self(account_id: str):
    """å¼ºåˆ¶æŠ“å– /v3/users/self å¹¶æ›´æ–°æŒä¹…åŒ–"""
    try:
        # æ”¯æŒ 'default' ä¼ªè´¦å·ï¼ˆæ¥è‡ª config.tomlï¼‰
        cookie = None
        acc = am_get_account_by_id(account_id, mask_cookie=False)
        if acc:
            cookie = acc.get("cookie", "")
        elif account_id == "default":
            cfg = load_config()
            auth = cfg.get('auth', {}) if cfg else {}
            cookie = auth.get('cookie', '')
        else:
            raise HTTPException(status_code=404, detail="è´¦å·ä¸å­˜åœ¨")

        if not cookie:
            raise HTTPException(status_code=400, detail="è´¦å·æœªé…ç½®Cookie")

        headers = build_stealth_headers(cookie)
        resp = requests.get('https://api.zsxq.com/v3/users/self', headers=headers, timeout=30)
        resp.raise_for_status()
        data = resp.json()
        if not data.get('succeeded'):
            raise HTTPException(status_code=400, detail="APIè¿”å›å¤±è´¥")

        rd = data.get('resp_data', {}) or {}
        user = rd.get('user', {}) or {}
        wechat = (rd.get('accounts', {}) or {}).get('wechat', {}) or {}

        self_info = {
            "uid": user.get("uid"),
            "name": user.get("name") or wechat.get("name"),
            "avatar_url": user.get("avatar_url") or wechat.get("avatar_url"),
            "location": user.get("location"),
            "user_sid": user.get("user_sid"),
            "grade": user.get("grade"),
        }
        db = get_account_info_db()
        db.upsert_self_info(account_id, self_info, raw_json=data)
        return {"self": db.get_self_info(account_id)}
    except HTTPException:
        raise
    except requests.RequestException as e:
        raise HTTPException(status_code=502, detail=f"ç½‘ç»œè¯·æ±‚å¤±è´¥: {str(e)}")
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"åˆ·æ–°è´¦å·ä¿¡æ¯å¤±è´¥: {str(e)}")

@app.get("/api/groups/{group_id}/self")
async def get_group_account_self(group_id: str):
    """è·å–ç¾¤ç»„å½“å‰ä½¿ç”¨è´¦å·çš„è‡ªæˆ‘ä¿¡æ¯ï¼ˆè‹¥æ— åˆ™å°è¯•æŠ“å–å¹¶ä¿å­˜ï¼‰"""
    try:
        summary = get_account_summary_for_group_auto(group_id)
        cookie = get_cookie_for_group(group_id)
        account_id = (summary or {}).get('id', 'default')

        if not cookie:
            raise HTTPException(status_code=400, detail="æœªæ‰¾åˆ°å¯ç”¨Cookieï¼Œè¯·å…ˆé…ç½®è´¦å·æˆ–é»˜è®¤Cookie")

        db = get_account_info_db()
        info = db.get_self_info(account_id)
        if info:
            return {"self": info}

        # æŠ“å–å¹¶å†™å…¥
        headers = build_stealth_headers(cookie)
        resp = requests.get('https://api.zsxq.com/v3/users/self', headers=headers, timeout=30)
        resp.raise_for_status()
        data = resp.json()
        if not data.get('succeeded'):
            raise HTTPException(status_code=400, detail="APIè¿”å›å¤±è´¥")

        rd = data.get('resp_data', {}) or {}
        user = rd.get('user', {}) or {}
        wechat = (rd.get('accounts', {}) or {}).get('wechat', {}) or {}

        self_info = {
            "uid": user.get("uid"),
            "name": user.get("name") or wechat.get("name"),
            "avatar_url": user.get("avatar_url") or wechat.get("avatar_url"),
            "location": user.get("location"),
            "user_sid": user.get("user_sid"),
            "grade": user.get("grade"),
        }
        db.upsert_self_info(account_id, self_info, raw_json=data)
        return {"self": db.get_self_info(account_id)}
    except HTTPException:
        raise
    except requests.RequestException as e:
        raise HTTPException(status_code=502, detail=f"ç½‘ç»œè¯·æ±‚å¤±è´¥: {str(e)}")
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"è·å–ç¾¤ç»„è´¦å·ä¿¡æ¯å¤±è´¥: {str(e)}")

@app.post("/api/groups/{group_id}/self/refresh")
async def refresh_group_account_self(group_id: str):
    """å¼ºåˆ¶æŠ“å–ç¾¤ç»„å½“å‰ä½¿ç”¨è´¦å·çš„è‡ªæˆ‘ä¿¡æ¯å¹¶æŒä¹…åŒ–"""
    try:
        summary = get_account_summary_for_group_auto(group_id)
        cookie = get_cookie_for_group(group_id)
        account_id = (summary or {}).get('id', 'default')

        if not cookie:
            raise HTTPException(status_code=400, detail="æœªæ‰¾åˆ°å¯ç”¨Cookieï¼Œè¯·å…ˆé…ç½®è´¦å·æˆ–é»˜è®¤Cookie")

        headers = build_stealth_headers(cookie)
        resp = requests.get('https://api.zsxq.com/v3/users/self', headers=headers, timeout=30)
        resp.raise_for_status()
        data = resp.json()
        if not data.get('succeeded'):
            raise HTTPException(status_code=400, detail="APIè¿”å›å¤±è´¥")

        rd = data.get('resp_data', {}) or {}
        user = rd.get('user', {}) or {}
        wechat = (rd.get('accounts', {}) or {}).get('wechat', {}) or {}

        self_info = {
            "uid": user.get("uid"),
            "name": user.get("name") or wechat.get("name"),
            "avatar_url": user.get("avatar_url") or wechat.get("avatar_url"),
            "location": user.get("location"),
            "user_sid": user.get("user_sid"),
            "grade": user.get("grade"),
        }
        db = get_account_info_db()
        db.upsert_self_info(account_id, self_info, raw_json=data)
        return {"self": db.get_self_info(account_id)}
    except HTTPException:
        raise
    except requests.RequestException as e:
        raise HTTPException(status_code=502, detail=f"ç½‘ç»œè¯·æ±‚å¤±è´¥: {str(e)}")
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"åˆ·æ–°ç¾¤ç»„è´¦å·ä¿¡æ¯å¤±è´¥: {str(e)}")

@app.get("/api/database/stats")
async def get_database_stats():
    """è·å–æ•°æ®åº“ç»Ÿè®¡ä¿¡æ¯"""
    try:
        # æ£€æŸ¥æ˜¯å¦å·²é…ç½®
        if not is_configured():
            return {
                "configured": False,
                "topic_database": {
                    "stats": {},
                    "timestamp_info": {
                        "total_topics": 0,
                        "oldest_timestamp": "",
                        "newest_timestamp": "",
                        "has_data": False
                    }
                },
                "file_database": {
                    "stats": {}
                }
            }

        crawler = get_crawler_safe()
        if not crawler:
            return {
                "configured": False,
                "topic_database": {
                    "stats": {},
                    "timestamp_info": {
                        "total_topics": 0,
                        "oldest_timestamp": "",
                        "newest_timestamp": "",
                        "has_data": False
                    }
                },
                "file_database": {
                    "stats": {}
                }
            }

        # è·å–è¯é¢˜æ•°æ®åº“ç»Ÿè®¡
        topic_stats = crawler.db.get_database_stats()
        timestamp_info = crawler.db.get_timestamp_range_info()

        # è·å–æ–‡ä»¶æ•°æ®åº“ç»Ÿè®¡
        file_downloader = crawler.get_file_downloader()
        file_stats = file_downloader.file_db.get_database_stats()

        return {
            "configured": True,
            "topic_database": {
                "stats": topic_stats,
                "timestamp_info": timestamp_info
            },
            "file_database": {
                "stats": file_stats
            }
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"è·å–æ•°æ®åº“ç»Ÿè®¡å¤±è´¥: {str(e)}")

@app.get("/api/tasks")
async def get_tasks():
    """è·å–æ‰€æœ‰ä»»åŠ¡çŠ¶æ€"""
    return list(current_tasks.values())

@app.get("/api/tasks/{task_id}")
async def get_task(task_id: str):
    """è·å–ç‰¹å®šä»»åŠ¡çŠ¶æ€"""
    if task_id not in current_tasks:
        raise HTTPException(status_code=404, detail="ä»»åŠ¡ä¸å­˜åœ¨")

    return current_tasks[task_id]

@app.post("/api/tasks/{task_id}/stop")
async def stop_task_api(task_id: str):
    """åœæ­¢ä»»åŠ¡"""
    if stop_task(task_id):
        return {"message": "ä»»åŠ¡åœæ­¢è¯·æ±‚å·²å‘é€", "task_id": task_id}
    else:
        raise HTTPException(status_code=404, detail="ä»»åŠ¡ä¸å­˜åœ¨æˆ–æ— æ³•åœæ­¢")

# åå°ä»»åŠ¡æ‰§è¡Œå‡½æ•°
def run_crawl_historical_task(task_id: str, group_id: str, pages: int, per_page: int, crawl_settings: CrawlHistoricalRequest = None):
    """åå°æ‰§è¡Œå†å²æ•°æ®çˆ¬å–ä»»åŠ¡"""
    try:
        # æ£€æŸ¥ä»»åŠ¡æ˜¯å¦è¢«åœæ­¢
        if is_task_stopped(task_id):
            return

        update_task(task_id, "running", f"å¼€å§‹çˆ¬å–å†å²æ•°æ® {pages} é¡µ...")
        add_task_log(task_id, f"ğŸš€ å¼€å§‹è·å–å†å²æ•°æ®ï¼Œ{pages} é¡µï¼Œæ¯é¡µ {per_page} æ¡")

        # æ£€æŸ¥ä»»åŠ¡æ˜¯å¦è¢«åœæ­¢
        if is_task_stopped(task_id):
            return

        # è®¾ç½®æ—¥å¿—å›è°ƒå‡½æ•°
        def log_callback(message: str):
            add_task_log(task_id, message)

        # è®¾ç½®åœæ­¢æ£€æŸ¥å‡½æ•°
        def stop_check():
            return is_task_stopped(task_id)

        # ä¸ºæ¯ä¸ªä»»åŠ¡åˆ›å»ºç‹¬ç«‹çš„çˆ¬è™«å®ä¾‹ï¼Œä½¿ç”¨ä¼ å…¥çš„group_id
        # è‡ªåŠ¨åŒ¹é…è¯¥ç¾¤ç»„æ‰€å±è´¦å·ï¼Œè·å–å¯¹åº”Cookie
        cookie = get_cookie_for_group(group_id)
        # ä½¿ç”¨ä¼ å…¥çš„group_idè€Œä¸æ˜¯é…ç½®æ–‡ä»¶ä¸­çš„å›ºå®šå€¼
        path_manager = get_db_path_manager()
        db_path = path_manager.get_topics_db_path(group_id)

        crawler = ZSXQInteractiveCrawler(cookie, group_id, db_path, log_callback)
        # è®¾ç½®åœæ­¢æ£€æŸ¥å‡½æ•°
        crawler.stop_check_func = stop_check

        # è®¾ç½®è‡ªå®šä¹‰é—´éš”å‚æ•°
        if crawl_settings:
            crawler.set_custom_intervals(
                crawl_interval_min=crawl_settings.crawlIntervalMin,
                crawl_interval_max=crawl_settings.crawlIntervalMax,
                long_sleep_interval_min=crawl_settings.longSleepIntervalMin,
                long_sleep_interval_max=crawl_settings.longSleepIntervalMax,
                pages_per_batch=crawl_settings.pagesPerBatch
            )

        # æ£€æŸ¥ä»»åŠ¡æ˜¯å¦åœ¨è®¾ç½®è¿‡ç¨‹ä¸­è¢«åœæ­¢
        if is_task_stopped(task_id):
            add_task_log(task_id, "ğŸ›‘ ä»»åŠ¡åœ¨åˆå§‹åŒ–è¿‡ç¨‹ä¸­è¢«åœæ­¢")
            return

        add_task_log(task_id, "ğŸ“¡ è¿æ¥åˆ°çŸ¥è¯†æ˜ŸçƒAPI...")
        add_task_log(task_id, "ğŸ” æ£€æŸ¥æ•°æ®åº“çŠ¶æ€...")

        # æ£€æŸ¥ä»»åŠ¡æ˜¯å¦è¢«åœæ­¢
        if is_task_stopped(task_id):
            return

        result = crawler.crawl_incremental(pages, per_page)

        # æ£€æŸ¥ä»»åŠ¡æ˜¯å¦è¢«åœæ­¢
        if is_task_stopped(task_id):
            return

        # æ£€æŸ¥æ˜¯å¦æ˜¯ä¼šå‘˜è¿‡æœŸé”™è¯¯
        if result and result.get('expired'):
            add_task_log(task_id, f"âŒ ä¼šå‘˜å·²è¿‡æœŸ: {result.get('message', 'æˆå‘˜ä½“éªŒå·²åˆ°æœŸ')}")
            update_task(task_id, "failed", "ä¼šå‘˜å·²è¿‡æœŸ", {"expired": True, "code": result.get('code'), "message": result.get('message')})
            return

        add_task_log(task_id, f"âœ… è·å–å®Œæˆï¼æ–°å¢è¯é¢˜: {result.get('new_topics', 0)}, æ›´æ–°è¯é¢˜: {result.get('updated_topics', 0)}")
        update_task(task_id, "completed", "å†å²æ•°æ®çˆ¬å–å®Œæˆ", result)
    except Exception as e:
        if not is_task_stopped(task_id):
            add_task_log(task_id, f"âŒ è·å–å¤±è´¥: {str(e)}")
            update_task(task_id, "failed", f"çˆ¬å–å¤±è´¥: {str(e)}")

def run_file_download_task(task_id: str, group_id: str, max_files: Optional[int], sort_by: str,
                          download_interval: float = 1.0, long_sleep_interval: float = 60.0,
                          files_per_batch: int = 10, download_interval_min: Optional[float] = None,
                          download_interval_max: Optional[float] = None,
                          long_sleep_interval_min: Optional[float] = None,
                          long_sleep_interval_max: Optional[float] = None):
    """åå°æ‰§è¡Œæ–‡ä»¶ä¸‹è½½ä»»åŠ¡"""
    try:
        update_task(task_id, "running", "å¼€å§‹æ–‡ä»¶ä¸‹è½½...")

        def log_callback(message: str):
            add_task_log(task_id, message)

        # è®¾ç½®åœæ­¢æ£€æŸ¥å‡½æ•°
        def stop_check():
            return is_task_stopped(task_id)

        # ä¸ºæ¯ä¸ªä»»åŠ¡åˆ›å»ºç‹¬ç«‹çš„æ–‡ä»¶ä¸‹è½½å™¨å®ä¾‹ï¼Œä½¿ç”¨ä¼ å…¥çš„group_id
        # è‡ªåŠ¨åŒ¹é…è¯¥ç¾¤ç»„æ‰€å±è´¦å·ï¼Œè·å–å¯¹åº”Cookie
        cookie = get_cookie_for_group(group_id)

        # ä½¿ç”¨ä¼ å…¥çš„group_idè€Œä¸æ˜¯é…ç½®æ–‡ä»¶ä¸­çš„å›ºå®šå€¼
        from zsxq_file_downloader import ZSXQFileDownloader
        from db_path_manager import get_db_path_manager

        path_manager = get_db_path_manager()
        db_path = path_manager.get_files_db_path(group_id)

        downloader = ZSXQFileDownloader(
            cookie=cookie,
            group_id=group_id,
            db_path=db_path,
            download_interval=download_interval,
            long_sleep_interval=long_sleep_interval,
            files_per_batch=files_per_batch,
            download_interval_min=download_interval_min,
            download_interval_max=download_interval_max,
            long_sleep_interval_min=long_sleep_interval_min,
            long_sleep_interval_max=long_sleep_interval_max
        )
        # è®¾ç½®æ—¥å¿—å›è°ƒå’Œåœæ­¢æ£€æŸ¥å‡½æ•°
        downloader.log_callback = log_callback
        downloader.stop_check_func = stop_check

        add_task_log(task_id, f"âš™ï¸ ä¸‹è½½é…ç½®:")
        add_task_log(task_id, f"   â±ï¸ å•æ¬¡ä¸‹è½½é—´éš”: {download_interval}ç§’")
        add_task_log(task_id, f"   ğŸ˜´ é•¿ä¼‘çœ é—´éš”: {long_sleep_interval}ç§’")
        add_task_log(task_id, f"   ğŸ“¦ æ‰¹æ¬¡å¤§å°: {files_per_batch}ä¸ªæ–‡ä»¶")

        # å°†ä¸‹è½½å™¨å®ä¾‹å­˜å‚¨åˆ°å…¨å±€å­—å…¸ä¸­
        global file_downloader_instances
        file_downloader_instances[task_id] = downloader

        # æ£€æŸ¥ä»»åŠ¡æ˜¯å¦åœ¨è®¾ç½®è¿‡ç¨‹ä¸­è¢«åœæ­¢
        if is_task_stopped(task_id):
            add_task_log(task_id, "ğŸ›‘ ä»»åŠ¡åœ¨åˆå§‹åŒ–è¿‡ç¨‹ä¸­è¢«åœæ­¢")
            return

        add_task_log(task_id, "ğŸ“¡ è¿æ¥åˆ°çŸ¥è¯†æ˜ŸçƒAPI...")
        add_task_log(task_id, "ğŸ” å¼€å§‹æ”¶é›†æ–‡ä»¶åˆ—è¡¨...")

        # å…ˆæ”¶é›†æ–‡ä»¶åˆ—è¡¨
        collect_result = downloader.collect_incremental_files()

        # æ£€æŸ¥ä»»åŠ¡æ˜¯å¦è¢«åœæ­¢
        if is_task_stopped(task_id):
            return

        add_task_log(task_id, f"ğŸ“Š æ–‡ä»¶æ”¶é›†å®Œæˆ: {collect_result}")
        add_task_log(task_id, "ğŸš€ å¼€å§‹ä¸‹è½½æ–‡ä»¶...")

        # æ ¹æ®æ’åºæ–¹å¼ä¸‹è½½æ–‡ä»¶
        if sort_by == "download_count":
            result = downloader.download_files_from_database(max_files=max_files, status_filter='pending')
        else:
            result = downloader.download_files_from_database(max_files=max_files, status_filter='pending')

        # æ£€æŸ¥ä»»åŠ¡æ˜¯å¦è¢«åœæ­¢
        if is_task_stopped(task_id):
            return

        add_task_log(task_id, f"âœ… æ–‡ä»¶ä¸‹è½½å®Œæˆï¼")
        update_task(task_id, "completed", "æ–‡ä»¶ä¸‹è½½å®Œæˆ", {"downloaded_files": result})
    except Exception as e:
        if not is_task_stopped(task_id):
            add_task_log(task_id, f"âŒ æ–‡ä»¶ä¸‹è½½å¤±è´¥: {str(e)}")
            update_task(task_id, "failed", f"æ–‡ä»¶ä¸‹è½½å¤±è´¥: {str(e)}")
    finally:
        # æ¸…ç†ä¸‹è½½å™¨å®ä¾‹
        if task_id in file_downloader_instances:
            del file_downloader_instances[task_id]

def run_single_file_download_task(task_id: str, group_id: str, file_id: int):
    """è¿è¡Œå•ä¸ªæ–‡ä»¶ä¸‹è½½ä»»åŠ¡"""
    try:
        update_task(task_id, "running", f"å¼€å§‹ä¸‹è½½æ–‡ä»¶ (ID: {file_id})...")

        def log_callback(message: str):
            add_task_log(task_id, message)

        # è®¾ç½®åœæ­¢æ£€æŸ¥å‡½æ•°
        def stop_check():
            return is_task_stopped(task_id)

        # åˆ›å»ºæ–‡ä»¶ä¸‹è½½å™¨å®ä¾‹
        # è‡ªåŠ¨åŒ¹é…è¯¥ç¾¤ç»„æ‰€å±è´¦å·ï¼Œè·å–å¯¹åº”Cookie
        cookie = get_cookie_for_group(group_id)

        from zsxq_file_downloader import ZSXQFileDownloader
        from db_path_manager import get_db_path_manager

        path_manager = get_db_path_manager()
        db_path = path_manager.get_files_db_path(group_id)

        downloader = ZSXQFileDownloader(
            cookie=cookie,
            group_id=group_id,
            db_path=db_path
        )
        # è®¾ç½®æ—¥å¿—å›è°ƒå’Œåœæ­¢æ£€æŸ¥å‡½æ•°
        downloader.log_callback = log_callback
        downloader.stop_check_func = stop_check

        # å°†ä¸‹è½½å™¨å®ä¾‹å­˜å‚¨åˆ°å…¨å±€å­—å…¸ä¸­
        global file_downloader_instances
        file_downloader_instances[task_id] = downloader

        # æ£€æŸ¥ä»»åŠ¡æ˜¯å¦åœ¨è®¾ç½®è¿‡ç¨‹ä¸­è¢«åœæ­¢
        if is_task_stopped(task_id):
            add_task_log(task_id, "ğŸ›‘ ä»»åŠ¡åœ¨åˆå§‹åŒ–è¿‡ç¨‹ä¸­è¢«åœæ­¢")
            return

        # å°è¯•ä»æ•°æ®åº“è·å–æ–‡ä»¶ä¿¡æ¯
        downloader.file_db.cursor.execute('''
            SELECT file_id, name, size, download_count
            FROM files
            WHERE file_id = ?
        ''', (file_id,))

        result = downloader.file_db.cursor.fetchone()

        if result:
            # å¦‚æœæ•°æ®åº“ä¸­æœ‰æ–‡ä»¶ä¿¡æ¯ï¼Œä½¿ç”¨æ•°æ®åº“ä¿¡æ¯
            file_id_db, file_name, file_size, download_count = result
            add_task_log(task_id, f"ğŸ“„ ä»æ•°æ®åº“è·å–æ–‡ä»¶ä¿¡æ¯: {file_name} ({file_size} bytes)")

            # æ„é€ æ–‡ä»¶ä¿¡æ¯ç»“æ„
            file_info = {
                'file': {
                    'id': file_id,
                    'name': file_name,
                    'size': file_size,
                    'download_count': download_count
                }
            }
        else:
            # å¦‚æœæ•°æ®åº“ä¸­æ²¡æœ‰æ–‡ä»¶ä¿¡æ¯ï¼Œç›´æ¥å°è¯•ä¸‹è½½
            add_task_log(task_id, f"ğŸ“„ æ•°æ®åº“ä¸­æ— æ–‡ä»¶ä¿¡æ¯ï¼Œå°è¯•ç›´æ¥ä¸‹è½½æ–‡ä»¶ ID: {file_id}")

            # æ„é€ æœ€å°æ–‡ä»¶ä¿¡æ¯ç»“æ„
            file_info = {
                'file': {
                    'id': file_id,
                    'name': f'file_{file_id}',  # ä½¿ç”¨é»˜è®¤æ–‡ä»¶å
                    'size': 0,  # æœªçŸ¥å¤§å°
                    'download_count': 0
                }
            }

        # ä¸‹è½½æ–‡ä»¶
        result = downloader.download_file(file_info)

        if result == "skipped":
            add_task_log(task_id, "âœ… æ–‡ä»¶å·²å­˜åœ¨ï¼Œè·³è¿‡ä¸‹è½½")
            update_task(task_id, "completed", "æ–‡ä»¶å·²å­˜åœ¨")
        elif result:
            add_task_log(task_id, "âœ… æ–‡ä»¶ä¸‹è½½æˆåŠŸ")

            # è·å–å®é™…ä¸‹è½½çš„æ–‡ä»¶ä¿¡æ¯
            actual_file_info = file_info['file']
            actual_file_name = actual_file_info.get('name', f'file_{file_id}')
            actual_file_size = actual_file_info.get('size', 0)

            # æ£€æŸ¥æœ¬åœ°æ–‡ä»¶è·å–å®é™…å¤§å°
            import os
            safe_filename = "".join(c for c in actual_file_name if c.isalnum() or c in '._-ï¼ˆï¼‰()[]{}')
            if not safe_filename:
                safe_filename = f"file_{file_id}"
            local_path = os.path.join(downloader.download_dir, safe_filename)

            if os.path.exists(local_path):
                actual_file_size = os.path.getsize(local_path)

            # æ›´æ–°æˆ–æ’å…¥æ–‡ä»¶çŠ¶æ€
            downloader.file_db.cursor.execute('''
                INSERT OR REPLACE INTO files
                (file_id, name, size, download_status, local_path, download_time, download_count)
                VALUES (?, ?, ?, 'downloaded', ?, CURRENT_TIMESTAMP, ?)
            ''', (file_id, actual_file_name, actual_file_size, local_path,
                  actual_file_info.get('download_count', 0)))
            downloader.file_db.conn.commit()

            update_task(task_id, "completed", "ä¸‹è½½æˆåŠŸ")
        else:
            add_task_log(task_id, "âŒ æ–‡ä»¶ä¸‹è½½å¤±è´¥")
            update_task(task_id, "failed", "ä¸‹è½½å¤±è´¥")

    except Exception as e:
        if not is_task_stopped(task_id):
            add_task_log(task_id, f"âŒ ä»»åŠ¡æ‰§è¡Œå¤±è´¥: {str(e)}")
            update_task(task_id, "failed", f"ä»»åŠ¡å¤±è´¥: {str(e)}")
    finally:
        # æ¸…ç†ä¸‹è½½å™¨å®ä¾‹
        if task_id in file_downloader_instances:
            del file_downloader_instances[task_id]

def run_single_file_download_task_with_info(task_id: str, group_id: str, file_id: int,
                                           file_name: Optional[str] = None, file_size: Optional[int] = None):
    """è¿è¡Œå•ä¸ªæ–‡ä»¶ä¸‹è½½ä»»åŠ¡ï¼ˆå¸¦æ–‡ä»¶ä¿¡æ¯ï¼‰"""
    try:
        update_task(task_id, "running", f"å¼€å§‹ä¸‹è½½æ–‡ä»¶ (ID: {file_id})...")

        def log_callback(message: str):
            add_task_log(task_id, message)

        # è®¾ç½®åœæ­¢æ£€æŸ¥å‡½æ•°
        def stop_check():
            return is_task_stopped(task_id)

        # åˆ›å»ºæ–‡ä»¶ä¸‹è½½å™¨å®ä¾‹
        # è‡ªåŠ¨åŒ¹é…è¯¥ç¾¤ç»„æ‰€å±è´¦å·ï¼Œè·å–å¯¹åº”Cookie
        cookie = get_cookie_for_group(group_id)

        from zsxq_file_downloader import ZSXQFileDownloader
        from db_path_manager import get_db_path_manager

        path_manager = get_db_path_manager()
        db_path = path_manager.get_files_db_path(group_id)

        downloader = ZSXQFileDownloader(
            cookie=cookie,
            group_id=group_id,
            db_path=db_path
        )
        # è®¾ç½®æ—¥å¿—å›è°ƒå’Œåœæ­¢æ£€æŸ¥å‡½æ•°
        downloader.log_callback = log_callback
        downloader.stop_check_func = stop_check

        # å°†ä¸‹è½½å™¨å®ä¾‹å­˜å‚¨åˆ°å…¨å±€å­—å…¸ä¸­
        global file_downloader_instances
        file_downloader_instances[task_id] = downloader

        # æ£€æŸ¥ä»»åŠ¡æ˜¯å¦åœ¨è®¾ç½®è¿‡ç¨‹ä¸­è¢«åœæ­¢
        if is_task_stopped(task_id):
            add_task_log(task_id, "ğŸ›‘ ä»»åŠ¡åœ¨åˆå§‹åŒ–è¿‡ç¨‹ä¸­è¢«åœæ­¢")
            return

        # æ„é€ æ–‡ä»¶ä¿¡æ¯ç»“æ„
        if file_name and file_size:
            add_task_log(task_id, f"ğŸ“„ ä½¿ç”¨æä¾›çš„æ–‡ä»¶ä¿¡æ¯: {file_name} ({file_size} bytes)")
            file_info = {
                'file': {
                    'id': file_id,
                    'name': file_name,
                    'size': file_size,
                    'download_count': 0
                }
            }
        else:
            # å°è¯•ä»æ•°æ®åº“è·å–æ–‡ä»¶ä¿¡æ¯
            downloader.file_db.cursor.execute('''
                SELECT file_id, name, size, download_count
                FROM files
                WHERE file_id = ?
            ''', (file_id,))

            result = downloader.file_db.cursor.fetchone()

            if result:
                file_id_db, db_file_name, db_file_size, download_count = result
                add_task_log(task_id, f"ğŸ“„ ä»æ•°æ®åº“è·å–æ–‡ä»¶ä¿¡æ¯: {db_file_name} ({db_file_size} bytes)")
                file_info = {
                    'file': {
                        'id': file_id,
                        'name': db_file_name,
                        'size': db_file_size,
                        'download_count': download_count
                    }
                }
            else:
                add_task_log(task_id, f"ğŸ“„ ç›´æ¥ä¸‹è½½æ–‡ä»¶ ID: {file_id}")
                file_info = {
                    'file': {
                        'id': file_id,
                        'name': f'file_{file_id}',
                        'size': 0,
                        'download_count': 0
                    }
                }

        # ä¸‹è½½æ–‡ä»¶
        result = downloader.download_file(file_info)

        if result == "skipped":
            add_task_log(task_id, "âœ… æ–‡ä»¶å·²å­˜åœ¨ï¼Œè·³è¿‡ä¸‹è½½")
            update_task(task_id, "completed", "æ–‡ä»¶å·²å­˜åœ¨")
        elif result:
            add_task_log(task_id, "âœ… æ–‡ä»¶ä¸‹è½½æˆåŠŸ")

            # è·å–å®é™…ä¸‹è½½çš„æ–‡ä»¶ä¿¡æ¯
            actual_file_info = file_info['file']
            actual_file_name = actual_file_info.get('name', f'file_{file_id}')
            actual_file_size = actual_file_info.get('size', 0)

            # æ£€æŸ¥æœ¬åœ°æ–‡ä»¶è·å–å®é™…å¤§å°
            import os
            safe_filename = "".join(c for c in actual_file_name if c.isalnum() or c in '._-ï¼ˆï¼‰()[]{}')
            if not safe_filename:
                safe_filename = f"file_{file_id}"
            local_path = os.path.join(downloader.download_dir, safe_filename)

            if os.path.exists(local_path):
                actual_file_size = os.path.getsize(local_path)

            # æ›´æ–°æˆ–æ’å…¥æ–‡ä»¶çŠ¶æ€
            downloader.file_db.cursor.execute('''
                INSERT OR REPLACE INTO files
                (file_id, name, size, download_status, local_path, download_time, download_count)
                VALUES (?, ?, ?, 'downloaded', ?, CURRENT_TIMESTAMP, ?)
            ''', (file_id, actual_file_name, actual_file_size, local_path,
                  actual_file_info.get('download_count', 0)))
            downloader.file_db.conn.commit()

            update_task(task_id, "completed", "ä¸‹è½½æˆåŠŸ")
        else:
            add_task_log(task_id, "âŒ æ–‡ä»¶ä¸‹è½½å¤±è´¥")
            update_task(task_id, "failed", "ä¸‹è½½å¤±è´¥")

    except Exception as e:
        if not is_task_stopped(task_id):
            add_task_log(task_id, f"âŒ ä»»åŠ¡æ‰§è¡Œå¤±è´¥: {str(e)}")
            update_task(task_id, "failed", f"ä»»åŠ¡å¤±è´¥: {str(e)}")
    finally:
        # æ¸…ç†ä¸‹è½½å™¨å®ä¾‹
        if task_id in file_downloader_instances:
            del file_downloader_instances[task_id]

# ç¾¤ç»„ç›¸å…³è¾…åŠ©å‡½æ•°
def fetch_groups_from_api(cookie: str) -> List[dict]:
    """ä»çŸ¥è¯†æ˜ŸçƒAPIè·å–ç¾¤ç»„åˆ—è¡¨"""
    import requests

    # å¦‚æœæ˜¯æµ‹è¯•Cookieï¼Œè¿”å›æ¨¡æ‹Ÿæ•°æ®
    if cookie == "test_cookie":
        return [
            {
                "group_id": 123456,
                "name": "æµ‹è¯•çŸ¥è¯†æ˜Ÿçƒç¾¤ç»„",
                "type": "public",
                "background_url": "https://via.placeholder.com/400x200/4f46e5/ffffff?text=Test+Group",
                "description": "è¿™æ˜¯ä¸€ä¸ªç”¨äºæµ‹è¯•çš„çŸ¥è¯†æ˜Ÿçƒç¾¤ç»„ï¼ŒåŒ…å«å„ç§æŠ€æœ¯è®¨è®ºå’Œå­¦ä¹ èµ„æºåˆ†äº«ã€‚",
                "create_time": "2023-01-15T10:30:00+08:00",
                "subscription_time": "2024-01-01T00:00:00+08:00",
                "expiry_time": "2024-12-31T23:59:59+08:00",
                "status": "active",
                "owner": {
                    "user_id": 1001,
                    "name": "æµ‹è¯•ç¾¤ä¸»",
                    "avatar_url": "https://via.placeholder.com/64x64/10b981/ffffff?text=Owner"
                },
                "statistics": {
                    "members_count": 1250,
                    "topics_count": 89,
                    "files_count": 156
                }
            },
            {
                "group_id": 789012,
                "name": "æŠ€æœ¯äº¤æµç¾¤",
                "type": "private",
                "background_url": "https://via.placeholder.com/400x200/059669/ffffff?text=Tech+Group",
                "description": "ä¸“æ³¨äºå‰ç«¯ã€åç«¯ã€ç§»åŠ¨å¼€å‘ç­‰æŠ€æœ¯é¢†åŸŸçš„æ·±åº¦äº¤æµä¸å®è·µåˆ†äº«ã€‚",
                "create_time": "2023-03-20T14:15:00+08:00",
                "subscription_time": "2024-02-15T00:00:00+08:00",
                "expiry_time": "2025-02-14T23:59:59+08:00",
                "status": "active",
                "owner": {
                    "user_id": 1002,
                    "name": "æŠ€æœ¯ä¸“å®¶",
                    "avatar_url": "https://via.placeholder.com/64x64/dc2626/ffffff?text=Tech"
                },
                "statistics": {
                    "members_count": 856,
                    "topics_count": 234,
                    "files_count": 67
                }
            },
            {
                "group_id": 345678,
                "name": "äº§å“è®¾è®¡è®¨è®º",
                "type": "public",
                "background_url": "https://via.placeholder.com/400x200/7c3aed/ffffff?text=Design+Group",
                "description": "UI/UXè®¾è®¡ã€äº§å“æ€ç»´ã€ç”¨æˆ·ä½“éªŒç­‰è®¾è®¡ç›¸å…³è¯é¢˜çš„ä¸“ä¸šè®¨è®ºç¤¾åŒºã€‚",
                "create_time": "2023-06-10T09:45:00+08:00",
                "subscription_time": "2024-03-01T00:00:00+08:00",
                "expiry_time": "2024-08-31T23:59:59+08:00",
                "status": "active",
                "owner": {
                    "user_id": 1003,
                    "name": "è®¾è®¡å¸ˆ",
                    "avatar_url": "https://via.placeholder.com/64x64/ea580c/ffffff?text=Design"
                },
                "statistics": {
                    "members_count": 432,
                    "topics_count": 156,
                    "files_count": 89
                }
            },
            {
                "group_id": 456789,
                "name": "åˆ›ä¸šæŠ•èµ„åœˆ",
                "type": "private",
                "background_url": "https://via.placeholder.com/400x200/dc2626/ffffff?text=Startup",
                "description": "åˆ›ä¸šè€…ã€æŠ•èµ„äººã€è¡Œä¸šä¸“å®¶çš„äº¤æµå¹³å°ï¼Œåˆ†äº«åˆ›ä¸šç»éªŒå’ŒæŠ•èµ„è§è§£ã€‚",
                "create_time": "2023-08-05T16:20:00+08:00",
                "subscription_time": "2024-01-10T00:00:00+08:00",
                "expiry_time": "2024-07-09T23:59:59+08:00",
                "status": "expiring_soon",
                "owner": {
                    "user_id": 1004,
                    "name": "æŠ•èµ„äºº",
                    "avatar_url": "https://via.placeholder.com/64x64/f59e0b/ffffff?text=VC"
                },
                "statistics": {
                    "members_count": 298,
                    "topics_count": 78,
                    "files_count": 45
                }
            },
            {
                "group_id": 567890,
                "name": "AIäººå·¥æ™ºèƒ½ç ”ç©¶",
                "type": "public",
                "background_url": "https://via.placeholder.com/400x200/06b6d4/ffffff?text=AI+Research",
                "description": "äººå·¥æ™ºèƒ½ã€æœºå™¨å­¦ä¹ ã€æ·±åº¦å­¦ä¹ ç­‰å‰æ²¿æŠ€æœ¯çš„ç ”ç©¶ä¸åº”ç”¨è®¨è®ºã€‚",
                "create_time": "2023-09-12T11:30:00+08:00",
                "subscription_time": "2024-04-01T00:00:00+08:00",
                "expiry_time": "2025-03-31T23:59:59+08:00",
                "status": "active",
                "owner": {
                    "user_id": 1005,
                    "name": "AIç ”ç©¶å‘˜",
                    "avatar_url": "https://via.placeholder.com/64x64/8b5cf6/ffffff?text=AI"
                },
                "statistics": {
                    "members_count": 1876,
                    "topics_count": 345,
                    "files_count": 234
                }
            }
        ]

    headers = build_stealth_headers(cookie)

    try:
        response = requests.get('https://api.zsxq.com/v2/groups', headers=headers, timeout=30)
        response.raise_for_status()

        data = response.json()
        if data.get('succeeded'):
            return data.get('resp_data', {}).get('groups', [])
        else:
            raise Exception(f"APIè¿”å›å¤±è´¥: {data.get('error_message', 'æœªçŸ¥é”™è¯¯')}")
    except requests.RequestException as e:
        raise Exception(f"ç½‘ç»œè¯·æ±‚å¤±è´¥: {str(e)}")
    except Exception as e:
        raise Exception(f"è·å–ç¾¤ç»„åˆ—è¡¨å¤±è´¥: {str(e)}")

# çˆ¬å–ç›¸å…³APIè·¯ç”±
@app.post("/api/crawl/historical/{group_id}")
async def crawl_historical(group_id: str, request: CrawlHistoricalRequest, background_tasks: BackgroundTasks):
    """çˆ¬å–å†å²æ•°æ®"""
    try:
        task_id = create_task("crawl_historical", f"çˆ¬å–å†å²æ•°æ® {request.pages} é¡µ (ç¾¤ç»„: {group_id})")

        # æ·»åŠ åå°ä»»åŠ¡
        background_tasks.add_task(run_crawl_historical_task, task_id, group_id, request.pages, request.per_page, request)

        return {"task_id": task_id, "message": "ä»»åŠ¡å·²åˆ›å»ºï¼Œæ­£åœ¨åå°æ‰§è¡Œ"}
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"åˆ›å»ºçˆ¬å–ä»»åŠ¡å¤±è´¥: {str(e)}")

@app.post("/api/crawl/all/{group_id}")
async def crawl_all(group_id: str, request: CrawlSettingsRequest, background_tasks: BackgroundTasks):
    """å…¨é‡çˆ¬å–æ‰€æœ‰å†å²æ•°æ®"""
    try:
        task_id = create_task("crawl_all", f"å…¨é‡çˆ¬å–æ‰€æœ‰å†å²æ•°æ® (ç¾¤ç»„: {group_id})")

        def run_crawl_all_task(task_id: str, group_id: str, crawl_settings: CrawlSettingsRequest = None):
            try:
                update_task(task_id, "running", "å¼€å§‹å…¨é‡çˆ¬å–...")
                add_task_log(task_id, "ğŸš€ å¼€å§‹å…¨é‡çˆ¬å–...")
                add_task_log(task_id, "âš ï¸ è­¦å‘Šï¼šæ­¤æ¨¡å¼å°†æŒç»­çˆ¬å–ç›´åˆ°æ²¡æœ‰æ•°æ®ï¼Œå¯èƒ½éœ€è¦å¾ˆé•¿æ—¶é—´")

                # åˆ›å»ºæ—¥å¿—å›è°ƒå‡½æ•°
                def log_callback(message):
                    add_task_log(task_id, message)

                # è®¾ç½®åœæ­¢æ£€æŸ¥å‡½æ•°
                def stop_check():
                    return is_task_stopped(task_id)

                # ä¸ºè¿™ä¸ªä»»åŠ¡åˆ›å»ºæ–°çš„çˆ¬è™«å®ä¾‹ï¼ˆå¸¦æ—¥å¿—å›è°ƒï¼‰ï¼Œä½¿ç”¨ä¼ å…¥çš„group_id
                config = load_config()
                auth_config = config.get('auth', {})
                default_cookie = auth_config.get('cookie', '')
                account = am_get_account_for_group(group_id)
                cookie = account.get('cookie', '') if account else default_cookie
                # ä½¿ç”¨ä¼ å…¥çš„group_idè€Œä¸æ˜¯é…ç½®æ–‡ä»¶ä¸­çš„å›ºå®šå€¼
                path_manager = get_db_path_manager()
                db_path = path_manager.get_topics_db_path(group_id)

                crawler = ZSXQInteractiveCrawler(cookie, group_id, db_path, log_callback)
                # è®¾ç½®åœæ­¢æ£€æŸ¥å‡½æ•°
                crawler.stop_check_func = stop_check

                # è®¾ç½®è‡ªå®šä¹‰é—´éš”å‚æ•°
                if crawl_settings:
                    crawler.set_custom_intervals(
                        crawl_interval_min=crawl_settings.crawlIntervalMin,
                        crawl_interval_max=crawl_settings.crawlIntervalMax,
                        long_sleep_interval_min=crawl_settings.longSleepIntervalMin,
                        long_sleep_interval_max=crawl_settings.longSleepIntervalMax,
                        pages_per_batch=crawl_settings.pagesPerBatch
                    )

                # æ£€æŸ¥ä»»åŠ¡æ˜¯å¦åœ¨è®¾ç½®è¿‡ç¨‹ä¸­è¢«åœæ­¢
                if is_task_stopped(task_id):
                    add_task_log(task_id, "ğŸ›‘ ä»»åŠ¡åœ¨åˆå§‹åŒ–è¿‡ç¨‹ä¸­è¢«åœæ­¢")
                    return

                add_task_log(task_id, "ğŸ“¡ è¿æ¥åˆ°çŸ¥è¯†æ˜ŸçƒAPI...")
                add_task_log(task_id, "ğŸ” æ£€æŸ¥æ•°æ®åº“çŠ¶æ€...")

                # æ£€æŸ¥ä»»åŠ¡æ˜¯å¦è¢«åœæ­¢
                if is_task_stopped(task_id):
                    return

                # è·å–æ•°æ®åº“çŠ¶æ€
                db_stats = crawler.db.get_database_stats()
                add_task_log(task_id, f"ğŸ“Š å½“å‰æ•°æ®åº“çŠ¶æ€: è¯é¢˜: {db_stats.get('topics', 0)}, ç”¨æˆ·: {db_stats.get('users', 0)}")

                # æ£€æŸ¥ä»»åŠ¡æ˜¯å¦è¢«åœæ­¢
                if is_task_stopped(task_id):
                    return

                add_task_log(task_id, "ğŸŒŠ å¼€å§‹æ— é™å†å²çˆ¬å–...")
                result = crawler.crawl_all_historical(per_page=20, auto_confirm=True)

                # æ£€æŸ¥ä»»åŠ¡æ˜¯å¦è¢«åœæ­¢
                if is_task_stopped(task_id):
                    return

                # æ£€æŸ¥æ˜¯å¦æ˜¯ä¼šå‘˜è¿‡æœŸé”™è¯¯
                if result and result.get('expired'):
                    add_task_log(task_id, f"âŒ ä¼šå‘˜å·²è¿‡æœŸ: {result.get('message', 'æˆå‘˜ä½“éªŒå·²åˆ°æœŸ')}")
                    update_task(task_id, "failed", "ä¼šå‘˜å·²è¿‡æœŸ", {"expired": True, "code": result.get('code'), "message": result.get('message')})
                    return

                add_task_log(task_id, f"ğŸ‰ å…¨é‡çˆ¬å–å®Œæˆï¼")
                add_task_log(task_id, f"ğŸ“Š æœ€ç»ˆç»Ÿè®¡: æ–°å¢è¯é¢˜: {result.get('new_topics', 0)}, æ›´æ–°è¯é¢˜: {result.get('updated_topics', 0)}, æ€»é¡µæ•°: {result.get('pages', 0)}")
                update_task(task_id, "completed", "å…¨é‡çˆ¬å–å®Œæˆ", result)
            except Exception as e:
                add_task_log(task_id, f"âŒ å…¨é‡çˆ¬å–å¤±è´¥: {str(e)}")
                update_task(task_id, "failed", f"å…¨é‡çˆ¬å–å¤±è´¥: {str(e)}")

        # æ·»åŠ åå°ä»»åŠ¡
        background_tasks.add_task(run_crawl_all_task, task_id, group_id, request)

        return {"task_id": task_id, "message": "ä»»åŠ¡å·²åˆ›å»ºï¼Œæ­£åœ¨åå°æ‰§è¡Œ"}
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"åˆ›å»ºå…¨é‡çˆ¬å–ä»»åŠ¡å¤±è´¥: {str(e)}")

@app.post("/api/crawl/incremental/{group_id}")
async def crawl_incremental(group_id: str, request: CrawlHistoricalRequest, background_tasks: BackgroundTasks):
    """å¢é‡çˆ¬å–å†å²æ•°æ®"""
    try:
        task_id = create_task("crawl_incremental", f"å¢é‡çˆ¬å–å†å²æ•°æ® {request.pages} é¡µ (ç¾¤ç»„: {group_id})")

        def run_crawl_incremental_task(task_id: str, group_id: str, pages: int, per_page: int, crawl_settings: CrawlHistoricalRequest = None):
            try:
                update_task(task_id, "running", "å¼€å§‹å¢é‡çˆ¬å–...")

                def log_callback(message: str):
                    add_task_log(task_id, message)

                # è®¾ç½®åœæ­¢æ£€æŸ¥å‡½æ•°
                def stop_check():
                    return is_task_stopped(task_id)

                # ä¸ºæ¯ä¸ªä»»åŠ¡åˆ›å»ºç‹¬ç«‹çš„çˆ¬è™«å®ä¾‹
                config = load_config()
                auth_config = config.get('auth', {})
                default_cookie = auth_config.get('cookie', '')
                account = am_get_account_for_group(group_id)
                cookie = account.get('cookie', '') if account else default_cookie
                # ä½¿ç”¨ä¼ å…¥çš„group_idè€Œä¸æ˜¯é…ç½®æ–‡ä»¶ä¸­çš„å›ºå®šå€¼
                path_manager = get_db_path_manager()
                db_path = path_manager.get_topics_db_path(group_id)

                crawler = ZSXQInteractiveCrawler(cookie, group_id, db_path, log_callback)
                # è®¾ç½®åœæ­¢æ£€æŸ¥å‡½æ•°
                crawler.stop_check_func = stop_check

                # è®¾ç½®è‡ªå®šä¹‰é—´éš”å‚æ•°
                if crawl_settings:
                    crawler.set_custom_intervals(
                        crawl_interval_min=crawl_settings.crawlIntervalMin,
                        crawl_interval_max=crawl_settings.crawlIntervalMax,
                        long_sleep_interval_min=crawl_settings.longSleepIntervalMin,
                        long_sleep_interval_max=crawl_settings.longSleepIntervalMax,
                        pages_per_batch=crawl_settings.pagesPerBatch
                    )

                # æ£€æŸ¥ä»»åŠ¡æ˜¯å¦åœ¨è®¾ç½®è¿‡ç¨‹ä¸­è¢«åœæ­¢
                if is_task_stopped(task_id):
                    add_task_log(task_id, "ğŸ›‘ ä»»åŠ¡åœ¨åˆå§‹åŒ–è¿‡ç¨‹ä¸­è¢«åœæ­¢")
                    return

                add_task_log(task_id, "ğŸ“¡ è¿æ¥åˆ°çŸ¥è¯†æ˜ŸçƒAPI...")
                add_task_log(task_id, "ğŸ” æ£€æŸ¥æ•°æ®åº“çŠ¶æ€...")

                result = crawler.crawl_incremental(pages, per_page)

                # æ£€æŸ¥ä»»åŠ¡æ˜¯å¦è¢«åœæ­¢
                if is_task_stopped(task_id):
                    return

                add_task_log(task_id, f"âœ… å¢é‡çˆ¬å–å®Œæˆï¼æ–°å¢è¯é¢˜: {result.get('new_topics', 0)}, æ›´æ–°è¯é¢˜: {result.get('updated_topics', 0)}")
                update_task(task_id, "completed", "å¢é‡çˆ¬å–å®Œæˆ", result)
            except Exception as e:
                if not is_task_stopped(task_id):
                    add_task_log(task_id, f"âŒ å¢é‡çˆ¬å–å¤±è´¥: {str(e)}")
                    update_task(task_id, "failed", f"å¢é‡çˆ¬å–å¤±è´¥: {str(e)}")

        # æ·»åŠ åå°ä»»åŠ¡
        background_tasks.add_task(run_crawl_incremental_task, task_id, group_id, request.pages, request.per_page, request)

        return {"task_id": task_id, "message": "ä»»åŠ¡å·²åˆ›å»ºï¼Œæ­£åœ¨åå°æ‰§è¡Œ"}
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"åˆ›å»ºå¢é‡çˆ¬å–ä»»åŠ¡å¤±è´¥: {str(e)}")

@app.post("/api/crawl/latest-until-complete/{group_id}")
async def crawl_latest_until_complete(group_id: str, request: CrawlSettingsRequest, background_tasks: BackgroundTasks):
    """è·å–æœ€æ–°è®°å½•ï¼šæ™ºèƒ½å¢é‡æ›´æ–°"""
    try:
        task_id = create_task("crawl_latest_until_complete", f"è·å–æœ€æ–°è®°å½• (ç¾¤ç»„: {group_id})")

        def run_crawl_latest_task(task_id: str, group_id: str, crawl_settings: CrawlSettingsRequest = None):
            try:
                update_task(task_id, "running", "å¼€å§‹è·å–æœ€æ–°è®°å½•...")

                def log_callback(message: str):
                    add_task_log(task_id, message)

                # è®¾ç½®åœæ­¢æ£€æŸ¥å‡½æ•°
                def stop_check():
                    return is_task_stopped(task_id)

                # ä¸ºæ¯ä¸ªä»»åŠ¡åˆ›å»ºç‹¬ç«‹çš„çˆ¬è™«å®ä¾‹ï¼Œä½¿ç”¨ä¼ å…¥çš„group_id
                config = load_config()
                auth_config = config.get('auth', {})
                default_cookie = auth_config.get('cookie', '')
                account = am_get_account_for_group(group_id)
                cookie = account.get('cookie', '') if account else default_cookie
                # ä½¿ç”¨ä¼ å…¥çš„group_idè€Œä¸æ˜¯é…ç½®æ–‡ä»¶ä¸­çš„å›ºå®šå€¼
                path_manager = get_db_path_manager()
                db_path = path_manager.get_topics_db_path(group_id)

                crawler = ZSXQInteractiveCrawler(cookie, group_id, db_path, log_callback)
                # è®¾ç½®åœæ­¢æ£€æŸ¥å‡½æ•°
                crawler.stop_check_func = stop_check

                # è®¾ç½®è‡ªå®šä¹‰é—´éš”å‚æ•°
                if crawl_settings:
                    crawler.set_custom_intervals(
                        crawl_interval_min=crawl_settings.crawlIntervalMin,
                        crawl_interval_max=crawl_settings.crawlIntervalMax,
                        long_sleep_interval_min=crawl_settings.longSleepIntervalMin,
                        long_sleep_interval_max=crawl_settings.longSleepIntervalMax,
                        pages_per_batch=crawl_settings.pagesPerBatch
                    )

                # æ£€æŸ¥ä»»åŠ¡æ˜¯å¦åœ¨è®¾ç½®è¿‡ç¨‹ä¸­è¢«åœæ­¢
                if is_task_stopped(task_id):
                    add_task_log(task_id, "ğŸ›‘ ä»»åŠ¡åœ¨åˆå§‹åŒ–è¿‡ç¨‹ä¸­è¢«åœæ­¢")
                    return

                add_task_log(task_id, "ğŸ“¡ è¿æ¥åˆ°çŸ¥è¯†æ˜ŸçƒAPI...")
                add_task_log(task_id, "ğŸ” æ£€æŸ¥æ•°æ®åº“çŠ¶æ€...")

                result = crawler.crawl_latest_until_complete()

                # æ£€æŸ¥ä»»åŠ¡æ˜¯å¦è¢«åœæ­¢
                if is_task_stopped(task_id):
                    return

                # æ£€æŸ¥æ˜¯å¦æ˜¯ä¼šå‘˜è¿‡æœŸé”™è¯¯
                if result and result.get('expired'):
                    add_task_log(task_id, f"âŒ ä¼šå‘˜å·²è¿‡æœŸ: {result.get('message', 'æˆå‘˜ä½“éªŒå·²åˆ°æœŸ')}")
                    update_task(task_id, "failed", "ä¼šå‘˜å·²è¿‡æœŸ", {"expired": True, "code": result.get('code'), "message": result.get('message')})
                    return

                add_task_log(task_id, f"âœ… è·å–æœ€æ–°è®°å½•å®Œæˆï¼æ–°å¢è¯é¢˜: {result.get('new_topics', 0)}, æ›´æ–°è¯é¢˜: {result.get('updated_topics', 0)}")
                update_task(task_id, "completed", "è·å–æœ€æ–°è®°å½•å®Œæˆ", result)
            except Exception as e:
                if not is_task_stopped(task_id):
                    add_task_log(task_id, f"âŒ è·å–æœ€æ–°è®°å½•å¤±è´¥: {str(e)}")
                    update_task(task_id, "failed", f"è·å–æœ€æ–°è®°å½•å¤±è´¥: {str(e)}")

        # æ·»åŠ åå°ä»»åŠ¡
        background_tasks.add_task(run_crawl_latest_task, task_id, group_id, request)

        return {"task_id": task_id, "message": "ä»»åŠ¡å·²åˆ›å»ºï¼Œæ­£åœ¨åå°æ‰§è¡Œ"}
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"åˆ›å»ºè·å–æœ€æ–°è®°å½•ä»»åŠ¡å¤±è´¥: {str(e)}")

# æ–‡ä»¶ç›¸å…³APIè·¯ç”±
@app.post("/api/files/collect/{group_id}")
async def collect_files(group_id: str, background_tasks: BackgroundTasks):
    """æ”¶é›†æ–‡ä»¶åˆ—è¡¨"""
    try:
        task_id = create_task("collect_files", "æ”¶é›†æ–‡ä»¶åˆ—è¡¨")

        def run_collect_files_task(task_id: str, group_id: str):
            try:
                update_task(task_id, "running", "å¼€å§‹æ”¶é›†æ–‡ä»¶åˆ—è¡¨...")

                def log_callback(message: str):
                    add_task_log(task_id, message)

                # è®¾ç½®åœæ­¢æ£€æŸ¥å‡½æ•°
                def stop_check():
                    return is_task_stopped(task_id)

                # ä¸ºæ¯ä¸ªä»»åŠ¡åˆ›å»ºç‹¬ç«‹çš„æ–‡ä»¶ä¸‹è½½å™¨å®ä¾‹
                config = load_config()
                auth_config = config.get('auth', {})
                default_cookie = auth_config.get('cookie', '')
                account = am_get_account_for_group(group_id)
                cookie = account.get('cookie', '') if account else default_cookie

                from zsxq_file_downloader import ZSXQFileDownloader
                from db_path_manager import get_db_path_manager

                path_manager = get_db_path_manager()
                db_path = path_manager.get_files_db_path(group_id)

                downloader = ZSXQFileDownloader(cookie, group_id, db_path)
                downloader.log_callback = log_callback
                downloader.stop_check_func = stop_check

                # å°†ä¸‹è½½å™¨å®ä¾‹å­˜å‚¨åˆ°å…¨å±€å­—å…¸ä¸­
                global file_downloader_instances
                file_downloader_instances[task_id] = downloader

                # æ£€æŸ¥ä»»åŠ¡æ˜¯å¦åœ¨è®¾ç½®è¿‡ç¨‹ä¸­è¢«åœæ­¢
                if is_task_stopped(task_id):
                    add_task_log(task_id, "ğŸ›‘ ä»»åŠ¡åœ¨åˆå§‹åŒ–è¿‡ç¨‹ä¸­è¢«åœæ­¢")
                    return

                add_task_log(task_id, "ğŸ“¡ è¿æ¥åˆ°çŸ¥è¯†æ˜ŸçƒAPI...")
                result = downloader.collect_incremental_files()

                # æ£€æŸ¥ä»»åŠ¡æ˜¯å¦è¢«åœæ­¢
                if is_task_stopped(task_id):
                    return

                add_task_log(task_id, f"âœ… æ–‡ä»¶åˆ—è¡¨æ”¶é›†å®Œæˆï¼")
                update_task(task_id, "completed", "æ–‡ä»¶åˆ—è¡¨æ”¶é›†å®Œæˆ", result)
            except Exception as e:
                if not is_task_stopped(task_id):
                    add_task_log(task_id, f"âŒ æ–‡ä»¶åˆ—è¡¨æ”¶é›†å¤±è´¥: {str(e)}")
                    update_task(task_id, "failed", f"æ–‡ä»¶åˆ—è¡¨æ”¶é›†å¤±è´¥: {str(e)}")
            finally:
                # æ¸…ç†ä¸‹è½½å™¨å®ä¾‹
                if task_id in file_downloader_instances:
                    del file_downloader_instances[task_id]

        # æ·»åŠ åå°ä»»åŠ¡
        background_tasks.add_task(run_collect_files_task, task_id, group_id)

        return {"task_id": task_id, "message": "ä»»åŠ¡å·²åˆ›å»ºï¼Œæ­£åœ¨åå°æ‰§è¡Œ"}
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"åˆ›å»ºæ–‡ä»¶æ”¶é›†ä»»åŠ¡å¤±è´¥: {str(e)}")

@app.post("/api/files/download/{group_id}")
async def download_files(group_id: str, request: FileDownloadRequest, background_tasks: BackgroundTasks):
    """ä¸‹è½½æ–‡ä»¶"""
    try:
        task_id = create_task("download_files", f"ä¸‹è½½æ–‡ä»¶ (æ’åº: {request.sort_by})")

        # æ·»åŠ åå°ä»»åŠ¡
        background_tasks.add_task(
            run_file_download_task,
            task_id,
            group_id,
            request.max_files,
            request.sort_by,
            request.download_interval,
            request.long_sleep_interval,
            request.files_per_batch,
            request.download_interval_min,
            request.download_interval_max,
            request.long_sleep_interval_min,
            request.long_sleep_interval_max
        )

        return {"task_id": task_id, "message": "ä»»åŠ¡å·²åˆ›å»ºï¼Œæ­£åœ¨åå°æ‰§è¡Œ"}
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"åˆ›å»ºæ–‡ä»¶ä¸‹è½½ä»»åŠ¡å¤±è´¥: {str(e)}")

@app.post("/api/files/download-single/{group_id}/{file_id}")
async def download_single_file(group_id: str, file_id: int, background_tasks: BackgroundTasks,
                              file_name: Optional[str] = None, file_size: Optional[int] = None):
    """ä¸‹è½½å•ä¸ªæ–‡ä»¶"""
    try:
        task_id = create_task("download_single_file", f"ä¸‹è½½å•ä¸ªæ–‡ä»¶ (ID: {file_id})")

        # æ·»åŠ åå°ä»»åŠ¡
        background_tasks.add_task(
            run_single_file_download_task_with_info,
            task_id,
            group_id,
            file_id,
            file_name,
            file_size
        )

        return {"task_id": task_id, "message": "å•ä¸ªæ–‡ä»¶ä¸‹è½½ä»»åŠ¡å·²åˆ›å»º"}
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"åˆ›å»ºå•ä¸ªæ–‡ä»¶ä¸‹è½½ä»»åŠ¡å¤±è´¥: {str(e)}")

@app.get("/api/files/status/{group_id}/{file_id}")
async def get_file_status(group_id: str, file_id: int):
    """è·å–æ–‡ä»¶ä¸‹è½½çŠ¶æ€"""
    try:
        crawler = get_crawler_for_group(group_id)
        downloader = crawler.get_file_downloader()

        # æŸ¥è¯¢æ–‡ä»¶ä¿¡æ¯
        downloader.file_db.cursor.execute('''
            SELECT name, size, download_status
            FROM files
            WHERE file_id = ?
        ''', (file_id,))

        result = downloader.file_db.cursor.fetchone()

        if not result:
            # æ–‡ä»¶ä¸åœ¨æ•°æ®åº“ä¸­ï¼Œæ£€æŸ¥æ˜¯å¦æœ‰åŒåæ–‡ä»¶åœ¨ä¸‹è½½ç›®å½•
            import os
            download_dir = downloader.download_dir

            # å°è¯•ä»è¯é¢˜è¯¦æƒ…ä¸­è·å–æ–‡ä»¶åï¼ˆè¿™é‡Œéœ€è¦é¢å¤–çš„é€»è¾‘ï¼‰
            # æš‚æ—¶è¿”å›æ–‡ä»¶ä¸å­˜åœ¨çš„çŠ¶æ€
            return {
                "file_id": file_id,
                "name": f"file_{file_id}",
                "size": 0,
                "download_status": "not_collected",
                "local_exists": False,
                "local_size": 0,
                "local_path": None,
                "is_complete": False,
                "message": "æ–‡ä»¶ä¿¡æ¯æœªæ”¶é›†ï¼Œè¯·å…ˆè¿è¡Œæ–‡ä»¶æ”¶é›†ä»»åŠ¡"
            }

        file_name, file_size, download_status = result

        # æ£€æŸ¥æœ¬åœ°æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        import os
        safe_filename = "".join(c for c in file_name if c.isalnum() or c in '._-ï¼ˆï¼‰()[]{}')
        if not safe_filename:
            safe_filename = f"file_{file_id}"

        download_dir = downloader.download_dir
        file_path = os.path.join(download_dir, safe_filename)

        local_exists = os.path.exists(file_path)
        local_size = os.path.getsize(file_path) if local_exists else 0

        return {
            "file_id": file_id,
            "name": file_name,
            "size": file_size,
            "download_status": download_status or "pending",
            "local_exists": local_exists,
            "local_size": local_size,
            "local_path": file_path if local_exists else None,
            "is_complete": local_exists and local_size == file_size
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"è·å–æ–‡ä»¶çŠ¶æ€å¤±è´¥: {str(e)}")

@app.get("/api/files/check-local/{group_id}")
async def check_local_file_status(group_id: str, file_name: str, file_size: int):
    """æ£€æŸ¥æœ¬åœ°æ–‡ä»¶çŠ¶æ€ï¼ˆä¸ä¾èµ–æ•°æ®åº“ï¼‰"""
    try:
        crawler = get_crawler_for_group(group_id)
        downloader = crawler.get_file_downloader()

        # æ¸…ç†æ–‡ä»¶å
        import os
        safe_filename = "".join(c for c in file_name if c.isalnum() or c in '._-ï¼ˆï¼‰()[]{}')
        if not safe_filename:
            safe_filename = file_name

        download_dir = downloader.download_dir
        file_path = os.path.join(download_dir, safe_filename)

        local_exists = os.path.exists(file_path)
        local_size = os.path.getsize(file_path) if local_exists else 0

        return {
            "file_name": file_name,
            "safe_filename": safe_filename,
            "expected_size": file_size,
            "local_exists": local_exists,
            "local_size": local_size,
            "local_path": file_path if local_exists else None,
            "is_complete": local_exists and (file_size == 0 or local_size == file_size),
            "download_dir": download_dir
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"æ£€æŸ¥æœ¬åœ°æ–‡ä»¶å¤±è´¥: {str(e)}")

@app.get("/api/files/stats/{group_id}")
async def get_file_stats(group_id: str):
    """è·å–æŒ‡å®šç¾¤ç»„çš„æ–‡ä»¶ç»Ÿè®¡ä¿¡æ¯"""
    crawler = None
    try:
        crawler = get_crawler_for_group(group_id)
        downloader = crawler.get_file_downloader()

        # è·å–æ–‡ä»¶æ•°æ®åº“ç»Ÿè®¡
        stats = downloader.file_db.get_database_stats()

        # è·å–ä¸‹è½½çŠ¶æ€ç»Ÿè®¡
        # é¦–å…ˆæ£€æŸ¥æ˜¯å¦æœ‰download_statusåˆ—
        downloader.file_db.cursor.execute("PRAGMA table_info(files)")
        columns = [col[1] for col in downloader.file_db.cursor.fetchall()]

        if 'download_status' in columns:
            # æ–°ç‰ˆæœ¬æ•°æ®åº“ï¼Œæœ‰download_statusåˆ—
            downloader.file_db.cursor.execute("""
                SELECT
                    COUNT(*) as total_files,
                    COUNT(CASE WHEN download_status = 'completed' THEN 1 END) as downloaded,
                    COUNT(CASE WHEN download_status = 'pending' THEN 1 END) as pending,
                    COUNT(CASE WHEN download_status = 'failed' THEN 1 END) as failed
                FROM files
            """)
            download_stats = downloader.file_db.cursor.fetchone()
        else:
            # æ—§ç‰ˆæœ¬æ•°æ®åº“ï¼Œæ²¡æœ‰download_statusåˆ—ï¼Œåªç»Ÿè®¡æ€»æ•°
            downloader.file_db.cursor.execute("SELECT COUNT(*) FROM files")
            total_files = downloader.file_db.cursor.fetchone()[0]
            download_stats = (total_files, 0, 0, 0)  # æ€»æ•°, å·²ä¸‹è½½, å¾…ä¸‹è½½, å¤±è´¥

        result = {
            "database_stats": stats,
            "download_stats": {
                "total_files": download_stats[0] if download_stats else 0,
                "downloaded": download_stats[1] if download_stats else 0,
                "pending": download_stats[2] if download_stats else 0,
                "failed": download_stats[3] if download_stats else 0
            }
        }

        return result
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"è·å–æ–‡ä»¶ç»Ÿè®¡å¤±è´¥: {str(e)}")
    finally:
        # ç¡®ä¿å…³é—­æ•°æ®åº“è¿æ¥
        if crawler:
            try:
                if hasattr(crawler, 'file_downloader') and crawler.file_downloader:
                    if hasattr(crawler.file_downloader, 'file_db') and crawler.file_downloader.file_db:
                        crawler.file_downloader.file_db.close()
                if hasattr(crawler, 'db') and crawler.db:
                    crawler.db.close()
                print(f"ğŸ”’ å·²å…³é—­ç¾¤ç»„ {group_id} çš„æ•°æ®åº“è¿æ¥")
            except Exception as e:
                print(f"âš ï¸ å…³é—­æ•°æ®åº“è¿æ¥æ—¶å‡ºé”™: {e}")

@app.post("/api/files/clear/{group_id}")
async def clear_file_database(group_id: str):
    """åˆ é™¤æŒ‡å®šç¾¤ç»„çš„æ–‡ä»¶æ•°æ®åº“æ–‡ä»¶"""
    try:
        path_manager = get_db_path_manager()
        db_path = path_manager.get_files_db_path(group_id)

        print(f"ğŸ—‘ï¸ å°è¯•åˆ é™¤æ–‡ä»¶æ•°æ®åº“: {db_path}")

        if os.path.exists(db_path):
            # å¼ºåˆ¶å…³é—­æ‰€æœ‰å¯èƒ½çš„æ•°æ®åº“è¿æ¥
            import gc
            import sqlite3

            # å°è¯•å¤šç§æ–¹å¼å…³é—­è¿æ¥
            try:
                # æ–¹å¼1ï¼šé€šè¿‡çˆ¬è™«å®ä¾‹å…³é—­
                crawler = get_crawler_for_group(group_id)
                downloader = crawler.get_file_downloader()
                if hasattr(downloader, 'file_db') and downloader.file_db:
                    downloader.file_db.close()
                if hasattr(crawler, 'db') and crawler.db:
                    crawler.db.close()
                print(f"âœ… å·²å…³é—­çˆ¬è™«å®ä¾‹çš„æ•°æ®åº“è¿æ¥")
            except Exception as e:
                print(f"âš ï¸ å…³é—­çˆ¬è™«æ•°æ®åº“è¿æ¥æ—¶å‡ºé”™: {e}")

            # æ–¹å¼2ï¼šå¼ºåˆ¶åƒåœ¾å›æ”¶
            gc.collect()

            # æ–¹å¼3ï¼šç­‰å¾…ä¸€å°æ®µæ—¶é—´è®©è¿æ¥é‡Šæ”¾
            import time
            time.sleep(0.5)

            # åˆ é™¤æ•°æ®åº“æ–‡ä»¶
            try:
                os.remove(db_path)
                print(f"âœ… æ–‡ä»¶æ•°æ®åº“å·²åˆ é™¤: {db_path}")

                # åŒæ—¶åˆ é™¤è¯¥ç¾¤ç»„çš„å›¾ç‰‡ç¼“å­˜
                try:
                    from image_cache_manager import get_image_cache_manager, clear_group_cache_manager
                    cache_manager = get_image_cache_manager(group_id)
                    success, message = cache_manager.clear_cache()
                    if success:
                        print(f"âœ… å›¾ç‰‡ç¼“å­˜å·²æ¸…ç©º: {message}")
                    else:
                        print(f"âš ï¸ æ¸…ç©ºå›¾ç‰‡ç¼“å­˜å¤±è´¥: {message}")
                    # æ¸…é™¤ç¼“å­˜ç®¡ç†å™¨å®ä¾‹
                    clear_group_cache_manager(group_id)
                except Exception as cache_error:
                    print(f"âš ï¸ æ¸…ç©ºå›¾ç‰‡ç¼“å­˜æ—¶å‡ºé”™: {cache_error}")

                return {"message": f"ç¾¤ç»„ {group_id} çš„æ–‡ä»¶æ•°æ®åº“å’Œå›¾ç‰‡ç¼“å­˜å·²åˆ é™¤"}
            except PermissionError as pe:
                print(f"âŒ æ–‡ä»¶è¢«å ç”¨ï¼Œæ— æ³•åˆ é™¤: {pe}")
                raise HTTPException(status_code=500, detail=f"æ–‡ä»¶è¢«å ç”¨ï¼Œæ— æ³•åˆ é™¤æ•°æ®åº“æ–‡ä»¶ã€‚è¯·ç¨åé‡è¯•ã€‚")
        else:
            print(f"â„¹ï¸ æ–‡ä»¶æ•°æ®åº“ä¸å­˜åœ¨: {db_path}")
            return {"message": f"ç¾¤ç»„ {group_id} çš„æ–‡ä»¶æ•°æ®åº“ä¸å­˜åœ¨"}
    except HTTPException:
        raise
    except Exception as e:
        print(f"âŒ åˆ é™¤æ–‡ä»¶æ•°æ®åº“å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=f"åˆ é™¤æ–‡ä»¶æ•°æ®åº“å¤±è´¥: {str(e)}")

@app.post("/api/topics/clear/{group_id}")
async def clear_topic_database(group_id: str):
    """åˆ é™¤æŒ‡å®šç¾¤ç»„çš„è¯é¢˜æ•°æ®åº“æ–‡ä»¶"""
    try:
        path_manager = get_db_path_manager()
        db_path = path_manager.get_topics_db_path(group_id)

        print(f"ğŸ—‘ï¸ å°è¯•åˆ é™¤è¯é¢˜æ•°æ®åº“: {db_path}")

        if os.path.exists(db_path):
            # å¼ºåˆ¶å…³é—­æ‰€æœ‰å¯èƒ½çš„æ•°æ®åº“è¿æ¥
            import gc
            import time

            # å°è¯•å¤šç§æ–¹å¼å…³é—­è¿æ¥
            try:
                # æ–¹å¼1ï¼šé€šè¿‡çˆ¬è™«å®ä¾‹å…³é—­
                crawler = get_crawler_for_group(group_id)
                if hasattr(crawler, 'db') and crawler.db:
                    crawler.db.close()
                if hasattr(crawler, 'file_downloader') and crawler.file_downloader:
                    if hasattr(crawler.file_downloader, 'file_db') and crawler.file_downloader.file_db:
                        crawler.file_downloader.file_db.close()
                print(f"âœ… å·²å…³é—­çˆ¬è™«å®ä¾‹çš„æ•°æ®åº“è¿æ¥")
            except Exception as e:
                print(f"âš ï¸ å…³é—­çˆ¬è™«æ•°æ®åº“è¿æ¥æ—¶å‡ºé”™: {e}")

            # æ–¹å¼2ï¼šå¼ºåˆ¶åƒåœ¾å›æ”¶
            gc.collect()

            # æ–¹å¼3ï¼šç­‰å¾…ä¸€å°æ®µæ—¶é—´è®©è¿æ¥é‡Šæ”¾
            time.sleep(0.5)

            # åˆ é™¤æ•°æ®åº“æ–‡ä»¶
            try:
                os.remove(db_path)
                print(f"âœ… è¯é¢˜æ•°æ®åº“å·²åˆ é™¤: {db_path}")

                # åŒæ—¶åˆ é™¤è¯¥ç¾¤ç»„çš„å›¾ç‰‡ç¼“å­˜
                try:
                    from image_cache_manager import get_image_cache_manager, clear_group_cache_manager
                    cache_manager = get_image_cache_manager(group_id)
                    success, message = cache_manager.clear_cache()
                    if success:
                        print(f"âœ… å›¾ç‰‡ç¼“å­˜å·²æ¸…ç©º: {message}")
                    else:
                        print(f"âš ï¸ æ¸…ç©ºå›¾ç‰‡ç¼“å­˜å¤±è´¥: {message}")
                    # æ¸…é™¤ç¼“å­˜ç®¡ç†å™¨å®ä¾‹
                    clear_group_cache_manager(group_id)
                except Exception as cache_error:
                    print(f"âš ï¸ æ¸…ç©ºå›¾ç‰‡ç¼“å­˜æ—¶å‡ºé”™: {cache_error}")

                return {"message": f"ç¾¤ç»„ {group_id} çš„è¯é¢˜æ•°æ®åº“å’Œå›¾ç‰‡ç¼“å­˜å·²åˆ é™¤"}
            except PermissionError as pe:
                print(f"âŒ æ–‡ä»¶è¢«å ç”¨ï¼Œæ— æ³•åˆ é™¤: {pe}")
                raise HTTPException(status_code=500, detail=f"æ–‡ä»¶è¢«å ç”¨ï¼Œæ— æ³•åˆ é™¤æ•°æ®åº“æ–‡ä»¶ã€‚è¯·ç¨åé‡è¯•ã€‚")
        else:
            print(f"â„¹ï¸ è¯é¢˜æ•°æ®åº“ä¸å­˜åœ¨: {db_path}")
            return {"message": f"ç¾¤ç»„ {group_id} çš„è¯é¢˜æ•°æ®åº“ä¸å­˜åœ¨"}
    except HTTPException:
        raise
    except Exception as e:
        print(f"âŒ åˆ é™¤è¯é¢˜æ•°æ®åº“å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=f"åˆ é™¤è¯é¢˜æ•°æ®åº“å¤±è´¥: {str(e)}")

# æ•°æ®æŸ¥è¯¢APIè·¯ç”±
@app.get("/api/topics")
async def get_topics(page: int = 1, per_page: int = 20, search: Optional[str] = None):
    """è·å–è¯é¢˜åˆ—è¡¨"""
    try:
        crawler = get_crawler()

        offset = (page - 1) * per_page

        # æ„å»ºæŸ¥è¯¢SQL
        if search:
            query = """
                SELECT topic_id, title, create_time, likes_count, comments_count, reading_count
                FROM topics
                WHERE title LIKE ?
                ORDER BY create_time DESC
                LIMIT ? OFFSET ?
            """
            params = (f"%{search}%", per_page, offset)
        else:
            query = """
                SELECT topic_id, title, create_time, likes_count, comments_count, reading_count
                FROM topics
                ORDER BY create_time DESC
                LIMIT ? OFFSET ?
            """
            params = (per_page, offset)

        crawler.db.cursor.execute(query, params)
        topics = crawler.db.cursor.fetchall()

        # è·å–æ€»æ•°
        if search:
            crawler.db.cursor.execute("SELECT COUNT(*) FROM topics WHERE title LIKE ?", (f"%{search}%",))
        else:
            crawler.db.cursor.execute("SELECT COUNT(*) FROM topics")
        total = crawler.db.cursor.fetchone()[0]

        return {
            "topics": [
                {
                    "topic_id": topic[0],
                    "title": topic[1],
                    "create_time": topic[2],
                    "likes_count": topic[3],
                    "comments_count": topic[4],
                    "reading_count": topic[5]
                }
                for topic in topics
            ],
            "pagination": {
                "page": page,
                "per_page": per_page,
                "total": total,
                "pages": (total + per_page - 1) // per_page
            }
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"è·å–è¯é¢˜åˆ—è¡¨å¤±è´¥: {str(e)}")

@app.get("/api/files/{group_id}")
async def get_files(group_id: str, page: int = 1, per_page: int = 20, status: Optional[str] = None):
    """è·å–æŒ‡å®šç¾¤ç»„çš„æ–‡ä»¶åˆ—è¡¨"""
    try:
        crawler = get_crawler_for_group(group_id)
        downloader = crawler.get_file_downloader()

        offset = (page - 1) * per_page

        # æ„å»ºæŸ¥è¯¢SQL
        if status:
            query = """
                SELECT file_id, name, size, download_count, create_time, download_status
                FROM files
                WHERE download_status = ?
                ORDER BY create_time DESC
                LIMIT ? OFFSET ?
            """
            params = (status, per_page, offset)
        else:
            query = """
                SELECT file_id, name, size, download_count, create_time, download_status
                FROM files
                ORDER BY create_time DESC
                LIMIT ? OFFSET ?
            """
            params = (per_page, offset)

        downloader.file_db.cursor.execute(query, params)
        files = downloader.file_db.cursor.fetchall()

        # è·å–æ€»æ•°
        if status:
            downloader.file_db.cursor.execute("SELECT COUNT(*) FROM files WHERE download_status = ?", (status,))
        else:
            downloader.file_db.cursor.execute("SELECT COUNT(*) FROM files")
        total = downloader.file_db.cursor.fetchone()[0]

        return {
            "files": [
                {
                    "file_id": file[0],
                    "name": file[1],
                    "size": file[2],
                    "download_count": file[3],
                    "create_time": file[4],
                    "download_status": file[5] if len(file) > 5 else "unknown"
                }
                for file in files
            ],
            "pagination": {
                "page": page,
                "per_page": per_page,
                "total": total,
                "pages": (total + per_page - 1) // per_page
            }
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"è·å–æ–‡ä»¶åˆ—è¡¨å¤±è´¥: {str(e)}")

# ç¾¤ç»„ç›¸å…³APIç«¯ç‚¹
@app.post("/api/local-groups/refresh")
async def refresh_local_groups():
    """
    æ‰‹åŠ¨åˆ·æ–°æœ¬åœ°ç¾¤ï¼ˆoutputï¼‰æ‰«æç¼“å­˜ï¼›ä¸æŠ›é”™ï¼Œå¼‚å¸¸æ—¶è¿”å›æ—§ç¼“å­˜ã€‚
    """
    try:
        ids = await asyncio.to_thread(scan_local_groups)
        return {"success": True, "count": len(ids), "groups": sorted(list(ids))}
    except Exception as e:
        cached = get_cached_local_group_ids(force_refresh=False) or set()
        # ä¸æŠ¥é”™ï¼Œè¿”å›é™çº§ç»“æœ
        return {"success": False, "count": len(cached), "groups": sorted(list(cached)), "error": str(e)}

@app.get("/api/groups")
async def get_groups():
    """è·å–ç¾¤ç»„åˆ—è¡¨ï¼šè´¦å·ç¾¤ âˆª æœ¬åœ°ç›®å½•ç¾¤ï¼ˆå»é‡åˆå¹¶ï¼‰"""
    try:
        # è‡ªåŠ¨æ„å»ºç¾¤ç»„â†’è´¦å·æ˜ å°„ï¼ˆå¤šè´¦å·æ”¯æŒï¼‰
        group_account_map = build_account_group_detection()
        local_ids = get_cached_local_group_ids(force_refresh=False)

        # è·å–â€œå½“å‰è´¦å·â€çš„ç¾¤åˆ—è¡¨ï¼ˆè‹¥æœªé…ç½®åˆ™è§†ä¸ºç©ºé›†åˆï¼‰
        groups_data: List[dict] = []
        try:
            if is_configured():
                config = load_config()
                auth_config = config.get('auth', {}) if config else {}
                cookie = auth_config.get('cookie', '') or ''
                if cookie and cookie != "your_cookie_here":
                    groups_data = fetch_groups_from_api(cookie)
        except Exception as e:
            # ä¸é˜»æ–­ï¼Œè®°å½•å‘Šè­¦
            print(f"âš ï¸ è·å–è´¦å·ç¾¤å¤±è´¥ï¼Œé™çº§ä¸ºæœ¬åœ°é›†åˆ: {e}")
            groups_data = []

        # ç»„è£…è´¦å·ä¾§ç¾¤ä¸ºå­—å…¸ï¼ˆid -> infoï¼‰
        by_id: Dict[int, dict] = {}

        for group in groups_data or []:
            # æå–ç”¨æˆ·ç‰¹å®šä¿¡æ¯
            user_specific = group.get('user_specific', {}) or {}
            validity = user_specific.get('validity', {}) or {}
            trial = user_specific.get('trial', {}) or {}

            # è¿‡æœŸä¿¡æ¯ä¸çŠ¶æ€
            actual_expiry_time = trial.get('end_time') or validity.get('end_time')
            is_trial = bool(trial.get('end_time'))

            status = None
            if actual_expiry_time:
                from datetime import datetime, timezone
                try:
                    end_time = datetime.fromisoformat(actual_expiry_time.replace('Z', '+00:00'))
                    now = datetime.now(timezone.utc)
                    days_until_expiry = (end_time - now).days
                    if days_until_expiry < 0:
                        status = 'expired'
                    elif days_until_expiry <= 7:
                        status = 'expiring_soon'
                    else:
                        status = 'active'
                except Exception:
                    pass

            gid = group.get('group_id')
            try:
                gid = int(gid)
            except Exception:
                continue

            info = {
                "group_id": gid,
                "name": group.get('name', ''),
                "type": group.get('type', ''),
                "background_url": group.get('background_url', ''),
                "owner": group.get('owner', {}) or {},
                "statistics": group.get('statistics', {}) or {},
                "status": status,
                "create_time": group.get('create_time'),
                "subscription_time": validity.get('begin_time'),
                "expiry_time": actual_expiry_time,
                "join_time": user_specific.get('join_time'),
                "last_active_time": user_specific.get('last_active_time'),
                "description": group.get('description', ''),
                "is_trial": is_trial,
                "trial_end_time": trial.get('end_time'),
                "membership_end_time": validity.get('end_time'),
                "account": group_account_map.get(str(gid)),
                "source": "account"
            }
            by_id[gid] = info

        # åˆå¹¶æœ¬åœ°ç›®å½•ç¾¤
        for gid in local_ids or []:
            try:
                gid_int = int(gid)
            except Exception:
                continue
            if gid_int in by_id:
                # æ ‡æ³¨æ¥æºä¸º account|local
                src = by_id[gid_int].get("source", "account")
                if "local" not in src:
                    by_id[gid_int]["source"] = "account|local"
            else:
                # ä»…å­˜åœ¨äºæœ¬åœ°
                by_id[gid_int] = {
                    "group_id": gid_int,
                    "name": "æœ¬åœ°ç¾¤ï¼ˆæœªç»‘å®šè´¦å·ï¼‰",
                    "type": "local",
                    "background_url": "",
                    "owner": {},
                    "statistics": {},
                    "status": None,
                    "create_time": None,
                    "subscription_time": None,
                    "expiry_time": None,
                    "join_time": None,
                    "last_active_time": None,
                    "description": "",
                    "is_trial": False,
                    "trial_end_time": None,
                    "membership_end_time": None,
                    "account": None,
                    "source": "local"
                }

        # æ’åºï¼šæŒ‰ç¾¤IDå‡åºï¼›å¦‚éœ€äºŒçº§æ’åºå†æŒ‰æ¥æºï¼ˆè´¦å·ä¼˜å…ˆï¼‰
        merged = [by_id[k] for k in sorted(by_id.keys())]

        return {
            "groups": merged,
            "total": len(merged)
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"è·å–ç¾¤ç»„åˆ—è¡¨å¤±è´¥: {str(e)}")

@app.get("/api/topics/{topic_id}/{group_id}")
async def get_topic_detail(topic_id: int, group_id: str):
    """è·å–è¯é¢˜è¯¦æƒ…"""
    try:
        crawler = get_crawler_for_group(group_id)
        topic_detail = crawler.db.get_topic_detail(topic_id)

        if not topic_detail:
            raise HTTPException(status_code=404, detail="è¯é¢˜ä¸å­˜åœ¨")

        return topic_detail
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"è·å–è¯é¢˜è¯¦æƒ…å¤±è´¥: {str(e)}")

@app.post("/api/topics/{topic_id}/{group_id}/refresh")
async def refresh_topic(topic_id: int, group_id: str):
    """å®æ—¶æ›´æ–°å•ä¸ªè¯é¢˜ä¿¡æ¯"""
    try:
        crawler = get_crawler_for_group(group_id)

        # ä½¿ç”¨çŸ¥è¯†æ˜ŸçƒAPIè·å–æœ€æ–°è¯é¢˜ä¿¡æ¯
        url = f"https://api.zsxq.com/v2/topics/{topic_id}/info"
        headers = crawler.get_stealth_headers()

        response = requests.get(url, headers=headers, timeout=30)

        if response.status_code == 200:
            data = response.json()
            if data.get('succeeded') and data.get('resp_data'):
                topic_data = data['resp_data']['topic']

                # åªæ›´æ–°è¯é¢˜çš„ç»Ÿè®¡ä¿¡æ¯ï¼Œé¿å…åˆ›å»ºé‡å¤è®°å½•
                success = crawler.db.update_topic_stats(topic_data)

                if not success:
                    return {"success": False, "message": "è¯é¢˜ä¸å­˜åœ¨æˆ–æ›´æ–°å¤±è´¥"}

                crawler.db.conn.commit()

                return {
                    "success": True,
                    "message": "è¯é¢˜ä¿¡æ¯å·²æ›´æ–°",
                    "updated_data": {
                        "likes_count": topic_data.get('likes_count', 0),
                        "comments_count": topic_data.get('comments_count', 0),
                        "reading_count": topic_data.get('reading_count', 0),
                        "readers_count": topic_data.get('readers_count', 0)
                    }
                }
            else:
                return {"success": False, "message": "APIè¿”å›æ•°æ®æ ¼å¼é”™è¯¯"}
        else:
            return {"success": False, "message": f"APIè¯·æ±‚å¤±è´¥: {response.status_code}"}

    except Exception as e:
        raise HTTPException(status_code=500, detail=f"æ›´æ–°è¯é¢˜å¤±è´¥: {str(e)}")

@app.post("/api/topics/{topic_id}/{group_id}/fetch-comments")
async def fetch_more_comments(topic_id: int, group_id: str):
    """æ‰‹åŠ¨è·å–è¯é¢˜çš„æ›´å¤šè¯„è®º"""
    try:
        crawler = get_crawler_for_group(group_id)

        # å…ˆè·å–è¯é¢˜åŸºæœ¬ä¿¡æ¯
        topic_detail = crawler.db.get_topic_detail(topic_id)
        if not topic_detail:
            raise HTTPException(status_code=404, detail="è¯é¢˜ä¸å­˜åœ¨")

        comments_count = topic_detail.get('comments_count', 0)
        if comments_count <= 8:
            return {
                "success": True,
                "message": f"è¯é¢˜åªæœ‰ {comments_count} æ¡è¯„è®ºï¼Œæ— éœ€è·å–æ›´å¤š",
                "comments_fetched": 0
            }

        # è·å–æ›´å¤šè¯„è®º
        try:
            additional_comments = crawler.fetch_all_comments(topic_id, comments_count)
            if additional_comments:
                crawler.db.import_additional_comments(topic_id, additional_comments)
                crawler.db.conn.commit()

                return {
                    "success": True,
                    "message": f"æˆåŠŸè·å–å¹¶å¯¼å…¥ {len(additional_comments)} æ¡è¯„è®º",
                    "comments_fetched": len(additional_comments)
                }
            else:
                return {
                    "success": False,
                    "message": "è·å–è¯„è®ºå¤±è´¥ï¼Œå¯èƒ½æ˜¯æƒé™é™åˆ¶æˆ–ç½‘ç»œé—®é¢˜",
                    "comments_fetched": 0
                }
        except Exception as e:
            return {
                "success": False,
                "message": f"è·å–è¯„è®ºæ—¶å‡ºé”™: {str(e)}",
                "comments_fetched": 0
            }

    except Exception as e:
        raise HTTPException(status_code=500, detail=f"è·å–æ›´å¤šè¯„è®ºå¤±è´¥: {str(e)}")

@app.delete("/api/topics/{topic_id}/{group_id}")
async def delete_single_topic(topic_id: int, group_id: int):
    """åˆ é™¤å•ä¸ªè¯é¢˜åŠå…¶æ‰€æœ‰å…³è”æ•°æ®"""
    crawler = None
    try:
        # ä½¿ç”¨æŒ‡å®šç¾¤ç»„çš„çˆ¬è™«å®ä¾‹ï¼Œä»¥ä¾¿å¤ç”¨å…¶æ•°æ®åº“è¿æ¥
        crawler = get_crawler_for_group(str(group_id))

        # æ£€æŸ¥è¯é¢˜æ˜¯å¦å­˜åœ¨ä¸”å±äºè¯¥ç¾¤ç»„
        crawler.db.cursor.execute('SELECT COUNT(*) FROM topics WHERE topic_id = ? AND group_id = ?', (topic_id, group_id))
        exists = crawler.db.cursor.fetchone()[0] > 0
        if not exists:
            return {"success": False, "message": "è¯é¢˜ä¸å­˜åœ¨"}

        # ä¾èµ–é¡ºåºåˆ é™¤å…³è”æ•°æ®
        tables_to_clean = [
            'user_liked_emojis',
            'like_emojis',
            'likes',
            'images',
            'comments',
            'answers',
            'questions',
            'articles',
            'talks',
            'topic_files',
            'topic_tags'
        ]

        for table in tables_to_clean:
            crawler.db.cursor.execute(f'DELETE FROM {table} WHERE topic_id = ?', (topic_id,))

        # æœ€ååˆ é™¤è¯é¢˜æœ¬èº«ï¼ˆé™å®šç¾¤ç»„ï¼‰
        crawler.db.cursor.execute('DELETE FROM topics WHERE topic_id = ? AND group_id = ?', (topic_id, group_id))

        deleted = crawler.db.cursor.rowcount
        crawler.db.conn.commit()

        return {"success": True, "deleted_topic_id": topic_id, "deleted": deleted > 0}
    except Exception as e:
        try:
            if crawler and hasattr(crawler, 'db') and crawler.db:
                crawler.db.conn.rollback()
        except Exception:
            pass
        raise HTTPException(status_code=500, detail=f"åˆ é™¤è¯é¢˜å¤±è´¥: {str(e)}")

# å•ä¸ªè¯é¢˜é‡‡é›† API
@app.post("/api/topics/fetch-single/{group_id}/{topic_id}")
async def fetch_single_topic(group_id: str, topic_id: int, fetch_comments: bool = True):
    """çˆ¬å–å¹¶å¯¼å…¥å•ä¸ªè¯é¢˜ï¼ˆç”¨äºç‰¹æ®Šè¯é¢˜æµ‹è¯•ï¼‰ï¼Œå¯é€‰æ‹‰å–å®Œæ•´è¯„è®º"""
    try:
        # ä½¿ç”¨è¯¥ç¾¤çš„è‡ªåŠ¨åŒ¹é…è´¦å·
        crawler = get_crawler_for_group(str(group_id))

        # æ‹‰å–è¯é¢˜è¯¦ç»†ä¿¡æ¯
        url = f"https://api.zsxq.com/v2/topics/{topic_id}/info"
        headers = crawler.get_stealth_headers()
        response = requests.get(url, headers=headers, timeout=30)

        if response.status_code != 200:
            raise HTTPException(status_code=response.status_code, detail="APIè¯·æ±‚å¤±è´¥")

        data = response.json()
        if not data.get("succeeded") or not data.get("resp_data"):
            raise HTTPException(status_code=400, detail="APIè¿”å›å¤±è´¥")

        topic = (data.get("resp_data", {}) or {}).get("topic", {}) or {}

        if not topic:
            raise HTTPException(status_code=404, detail="æœªè·å–åˆ°æœ‰æ•ˆè¯é¢˜æ•°æ®")

        # æ ¡éªŒè¯é¢˜æ‰€å±ç¾¤ç»„ä¸€è‡´æ€§
        topic_group_id = str((topic.get("group") or {}).get("group_id", ""))
        if topic_group_id and topic_group_id != str(group_id):
            raise HTTPException(status_code=400, detail="è¯¥è¯é¢˜ä¸å±äºå½“å‰ç¾¤ç»„")

        # åˆ¤æ–­è¯é¢˜æ˜¯å¦å·²å­˜åœ¨
        crawler.db.cursor.execute('SELECT topic_id FROM topics WHERE topic_id = ?', (topic_id,))
        existed = crawler.db.cursor.fetchone() is not None

        # å¯¼å…¥è¯é¢˜å®Œæ•´æ•°æ®
        crawler.db.import_topic_data(topic)
        crawler.db.conn.commit()

        # å¯é€‰ï¼šè·å–å®Œæ•´è¯„è®º
        comments_fetched = 0
        if fetch_comments:
            comments_count = topic.get("comments_count", 0) or 0
            if comments_count > 0:
                try:
                    additional_comments = crawler.fetch_all_comments(topic_id, comments_count)
                    if additional_comments:
                        crawler.db.import_additional_comments(topic_id, additional_comments)
                        crawler.db.conn.commit()
                        comments_fetched = len(additional_comments)
                except Exception as e:
                    # ä¸é˜»å¡ä¸»æµç¨‹
                    print(f"âš ï¸ å•è¯é¢˜è¯„è®ºè·å–å¤±è´¥: {e}")

        return {
            "success": True,
            "topic_id": topic_id,
            "group_id": int(group_id),
            "imported": "updated" if existed else "created",
            "comments_fetched": comments_fetched
        }
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"å•ä¸ªè¯é¢˜é‡‡é›†å¤±è´¥: {str(e)}")

# æ ‡ç­¾ç›¸å…³APIç«¯ç‚¹
@app.get("/api/groups/{group_id}/tags")
async def get_group_tags(group_id: str):
    """è·å–æŒ‡å®šç¾¤ç»„çš„æ‰€æœ‰æ ‡ç­¾"""
    try:
        crawler = get_crawler_for_group(group_id)
        tags = crawler.db.get_tags_by_group(int(group_id))
        
        return {
            "tags": tags,
            "total": len(tags)
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"è·å–æ ‡ç­¾åˆ—è¡¨å¤±è´¥: {str(e)}")

@app.get("/api/groups/{group_id}/tags/{tag_id}/topics")
async def get_topics_by_tag(group_id: int, tag_id: int, page: int = 1, per_page: int = 20):
    """æ ¹æ®æ ‡ç­¾è·å–æŒ‡å®šç¾¤ç»„çš„è¯é¢˜åˆ—è¡¨"""
    try:
        # ä½¿ç”¨æŒ‡å®šç¾¤ç»„çš„çˆ¬è™«å®ä¾‹
        crawler = get_crawler_for_group(str(group_id))
        
        # éªŒè¯æ ‡ç­¾æ˜¯å¦å­˜åœ¨äºè¯¥ç¾¤ç»„ä¸­
        crawler.db.cursor.execute('SELECT COUNT(*) FROM tags WHERE tag_id = ? AND group_id = ?', (tag_id, group_id))
        tag_count = crawler.db.cursor.fetchone()[0]
        
        if tag_count == 0:
            raise HTTPException(status_code=404, detail="æ ‡ç­¾åœ¨è¯¥ç¾¤ç»„ä¸­ä¸å­˜åœ¨")
            
        result = crawler.db.get_topics_by_tag(tag_id, page, per_page)
        
        return result
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"æ ¹æ®æ ‡ç­¾è·å–è¯é¢˜å¤±è´¥: {str(e)}")

@app.get("/api/proxy-image")
async def proxy_image(url: str, group_id: str = None):
    """ä»£ç†å›¾ç‰‡è¯·æ±‚ï¼Œæ”¯æŒæœ¬åœ°ç¼“å­˜"""
    try:
        cache_manager = get_image_cache_manager(group_id)

        # æ£€æŸ¥æ˜¯å¦å·²ç¼“å­˜
        if cache_manager.is_cached(url):
            cached_path = cache_manager.get_cached_path(url)
            if cached_path and cached_path.exists():
                # è¿”å›ç¼“å­˜çš„å›¾ç‰‡
                content_type = mimetypes.guess_type(str(cached_path))[0] or 'image/jpeg'

                with open(cached_path, 'rb') as f:
                    content = f.read()

                return Response(
                    content=content,
                    media_type=content_type,
                    headers={
                        'Cache-Control': 'public, max-age=86400',  # ç¼“å­˜24å°æ—¶
                        'Access-Control-Allow-Origin': '*',
                        'X-Cache-Status': 'HIT'
                    }
                )

        # ä¸‹è½½å¹¶ç¼“å­˜å›¾ç‰‡
        success, cached_path, error = cache_manager.download_and_cache(url)

        if success and cached_path and cached_path.exists():
            content_type = mimetypes.guess_type(str(cached_path))[0] or 'image/jpeg'

            with open(cached_path, 'rb') as f:
                content = f.read()

            return Response(
                content=content,
                media_type=content_type,
                headers={
                    'Cache-Control': 'public, max-age=86400',
                    'Access-Control-Allow-Origin': '*',
                    'X-Cache-Status': 'MISS'
                }
            )
        else:
            raise HTTPException(status_code=404, detail=f"å›¾ç‰‡åŠ è½½å¤±è´¥: {error}")

    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ä»£ç†å›¾ç‰‡å¤±è´¥: {str(e)}")


@app.get("/api/cache/images/info/{group_id}")
async def get_image_cache_info(group_id: str):
    """è·å–æŒ‡å®šç¾¤ç»„çš„å›¾ç‰‡ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯"""
    try:
        cache_manager = get_image_cache_manager(group_id)
        return cache_manager.get_cache_info()
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"è·å–ç¼“å­˜ä¿¡æ¯å¤±è´¥: {str(e)}")


@app.delete("/api/cache/images/{group_id}")
async def clear_image_cache(group_id: str):
    """æ¸…ç©ºæŒ‡å®šç¾¤ç»„çš„å›¾ç‰‡ç¼“å­˜"""
    try:
        cache_manager = get_image_cache_manager(group_id)
        success, message = cache_manager.clear_cache()

        if success:
            return {"success": True, "message": message}
        else:
            raise HTTPException(status_code=500, detail=message)
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"æ¸…ç©ºç¼“å­˜å¤±è´¥: {str(e)}")


@app.get("/api/settings/crawl")
async def get_crawl_settings():
    """è·å–è¯é¢˜çˆ¬å–è®¾ç½®"""
    try:
        # è¿”å›é»˜è®¤è®¾ç½®
        return {
            "crawl_interval_min": 2.0,
            "crawl_interval_max": 5.0,
            "long_sleep_interval_min": 180.0,
            "long_sleep_interval_max": 300.0,
            "pages_per_batch": 15
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"è·å–çˆ¬å–è®¾ç½®å¤±è´¥: {str(e)}")


@app.post("/api/settings/crawl")
async def update_crawl_settings(settings: dict):
    """æ›´æ–°è¯é¢˜çˆ¬å–è®¾ç½®"""
    try:
        # è¿™é‡Œå¯ä»¥å°†è®¾ç½®ä¿å­˜åˆ°é…ç½®æ–‡ä»¶æˆ–æ•°æ®åº“
        # ç›®å‰åªæ˜¯è¿”å›æˆåŠŸï¼Œå®é™…è®¾ç½®é€šè¿‡APIå‚æ•°ä¼ é€’
        return {"success": True, "message": "çˆ¬å–è®¾ç½®å·²æ›´æ–°"}
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"æ›´æ–°çˆ¬å–è®¾ç½®å¤±è´¥: {str(e)}")


@app.get("/api/groups/{group_id}/info")
async def get_group_info(group_id: str):
    """è·å–ç¾¤ç»„ä¿¡æ¯ï¼ˆå¸¦æœ¬åœ°å›é€€ï¼Œé¿å…401/500å¯¼è‡´å‰ç«¯æŠ¥é”™ï¼‰"""
    try:
        # è‡ªåŠ¨åŒ¹é…è¯¥ç¾¤ç»„æ‰€å±è´¦å·ï¼Œè·å–å¯¹åº”Cookie
        cookie = get_cookie_for_group(group_id)

        # æœ¬åœ°å›é€€æ•°æ®æ„é€ ï¼ˆä¸è®¿é—®å®˜æ–¹APIï¼‰
        def build_fallback(source: str = "fallback", note: str = None) -> dict:
            files_count = 0
            try:
                crawler = get_crawler_for_group(group_id)
                downloader = crawler.get_file_downloader()
                try:
                    downloader.file_db.cursor.execute("SELECT COUNT(*) FROM files")
                    row = downloader.file_db.cursor.fetchone()
                    files_count = (row[0] or 0) if row else 0
                except Exception:
                    files_count = 0
            except Exception:
                files_count = 0

            try:
                gid = int(group_id)
            except Exception:
                gid = group_id

            result = {
                "group_id": gid,
                "name": f"ç¾¤ç»„ {group_id}",
                "description": "",
                "statistics": {"files": {"count": files_count}},
                "background_url": None,
                "account": am_get_account_summary_for_group(group_id),
                "source": source,
            }
            if note:
                result["note"] = note
            return result

        # è‹¥æ²¡æœ‰å¯ç”¨ Cookieï¼Œç›´æ¥è¿”å›æœ¬åœ°å›é€€ï¼Œé¿å…æŠ› 400/500
        if not cookie:
            return build_fallback(note="no_cookie")

        # è°ƒç”¨å®˜æ–¹æ¥å£
        url = f"https://api.zsxq.com/v2/groups/{group_id}"
        headers = {
            'Cookie': cookie,
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }

        response = requests.get(url, headers=headers, timeout=30)

        if response.status_code == 200:
            data = response.json()
            if data.get('succeeded'):
                group_data = data.get('resp_data', {}).get('group', {})
                return {
                    "group_id": group_data.get('group_id'),
                    "name": group_data.get('name'),
                    "description": group_data.get('description'),
                    "statistics": group_data.get('statistics', {}),
                    "background_url": group_data.get('background_url'),
                    "account": am_get_account_summary_for_group(group_id),
                    "source": "remote"
                }
            # å®˜æ–¹è¿”å›é succeededï¼Œä¹Ÿèµ°å›é€€
            return build_fallback(note="remote_response_failed")
        else:
            # æˆæƒå¤±è´¥/æƒé™ä¸è¶³ â†’ ä½¿ç”¨æœ¬åœ°å›é€€ï¼ˆ200è¿”å›ï¼Œå‡å°‘å‰ç«¯å‘Šè­¦ï¼‰
            if response.status_code in (401, 403):
                return build_fallback(note=f"remote_api_{response.status_code}")
            # å…¶ä»–çŠ¶æ€ç ä¹Ÿå›é€€
            return build_fallback(note=f"remote_api_{response.status_code}")

    except Exception:
        # ä»»ä½•å¼‚å¸¸éƒ½å›é€€ä¸ºæœ¬åœ°ä¿¡æ¯ï¼Œé¿å… 500
        return build_fallback(note="exception_fallback")

@app.get("/api/groups/{group_id}/topics")
async def get_group_topics(group_id: int, page: int = 1, per_page: int = 20, search: Optional[str] = None):
    """è·å–æŒ‡å®šç¾¤ç»„çš„è¯é¢˜åˆ—è¡¨"""
    try:
        # ä½¿ç”¨æŒ‡å®šç¾¤ç»„çš„çˆ¬è™«å®ä¾‹
        crawler = get_crawler_for_group(str(group_id))

        offset = (page - 1) * per_page

        # æ„å»ºæŸ¥è¯¢SQL - åŒ…å«æ‰€æœ‰å†…å®¹ç±»å‹
        if search:
            query = """
                SELECT
                    t.topic_id, t.title, t.create_time, t.likes_count, t.comments_count,
                    t.reading_count, t.type, t.digested, t.sticky,
                    q.text as question_text,
                    a.text as answer_text,
                    tk.text as talk_text,
                    u.user_id, u.name, u.avatar_url, t.imported_at
                FROM topics t
                LEFT JOIN questions q ON t.topic_id = q.topic_id
                LEFT JOIN answers a ON t.topic_id = a.topic_id
                LEFT JOIN talks tk ON t.topic_id = tk.topic_id
                LEFT JOIN users u ON tk.owner_user_id = u.user_id
                WHERE t.group_id = ? AND (t.title LIKE ? OR q.text LIKE ? OR tk.text LIKE ?)
                ORDER BY t.create_time DESC
                LIMIT ? OFFSET ?
            """
            params = (group_id, f"%{search}%", f"%{search}%", f"%{search}%", per_page, offset)
        else:
            query = """
                SELECT
                    t.topic_id, t.title, t.create_time, t.likes_count, t.comments_count,
                    t.reading_count, t.type, t.digested, t.sticky,
                    q.text as question_text,
                    a.text as answer_text,
                    tk.text as talk_text,
                    u.user_id, u.name, u.avatar_url, t.imported_at
                FROM topics t
                LEFT JOIN questions q ON t.topic_id = q.topic_id
                LEFT JOIN answers a ON t.topic_id = a.topic_id
                LEFT JOIN talks tk ON t.topic_id = tk.topic_id
                LEFT JOIN users u ON tk.owner_user_id = u.user_id
                WHERE t.group_id = ?
                ORDER BY t.create_time DESC
                LIMIT ? OFFSET ?
            """
            params = (group_id, per_page, offset)

        crawler.db.cursor.execute(query, params)
        topics = crawler.db.cursor.fetchall()

        # è·å–æ€»æ•°
        if search:
            crawler.db.cursor.execute("SELECT COUNT(*) FROM topics WHERE group_id = ? AND title LIKE ?", (group_id, f"%{search}%"))
        else:
            crawler.db.cursor.execute("SELECT COUNT(*) FROM topics WHERE group_id = ?", (group_id,))
        total = crawler.db.cursor.fetchone()[0]

        # å¤„ç†è¯é¢˜æ•°æ®
        topics_list = []
        for topic in topics:
            topic_data = {
                "topic_id": topic[0],
                "title": topic[1],
                "create_time": topic[2],
                "likes_count": topic[3],
                "comments_count": topic[4],
                "reading_count": topic[5],
                "type": topic[6],
                "digested": bool(topic[7]) if topic[7] is not None else False,
                "sticky": bool(topic[8]) if topic[8] is not None else False,
                "imported_at": topic[15] if len(topic) > 15 else None  # è·å–æ—¶é—´
            }

            # æ·»åŠ å†…å®¹æ–‡æœ¬
            if topic[6] == 'q&a':
                # é—®ç­”ç±»å‹è¯é¢˜
                topic_data['question_text'] = topic[9] if topic[9] else ''
                topic_data['answer_text'] = topic[10] if topic[10] else ''
            else:
                # å…¶ä»–ç±»å‹è¯é¢˜ï¼ˆtalkã€articleç­‰ï¼‰
                topic_data['talk_text'] = topic[11] if topic[11] else ''
                if topic[12]:  # æœ‰ä½œè€…ä¿¡æ¯
                    topic_data['author'] = {
                        'user_id': topic[12],
                        'name': topic[13],
                        'avatar_url': topic[14]
                    }

            topics_list.append(topic_data)

        return {
            "topics": topics_list,
            "pagination": {
                "page": page,
                "per_page": per_page,
                "total": total,
                "pages": (total + per_page - 1) // per_page
            }
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"è·å–ç¾¤ç»„è¯é¢˜å¤±è´¥: {str(e)}")

@app.get("/api/groups/{group_id}/stats")
async def get_group_stats(group_id: int):
    """è·å–æŒ‡å®šç¾¤ç»„çš„ç»Ÿè®¡ä¿¡æ¯"""
    try:
        # ä½¿ç”¨æŒ‡å®šç¾¤ç»„çš„çˆ¬è™«å®ä¾‹
        crawler = get_crawler_for_group(str(group_id))
        cursor = crawler.db.cursor

        # è·å–è¯é¢˜ç»Ÿè®¡
        cursor.execute("SELECT COUNT(*) FROM topics WHERE group_id = ?", (group_id,))
        topics_count = cursor.fetchone()[0]

        # è·å–ç”¨æˆ·ç»Ÿè®¡ - ä»talksè¡¨è·å–ï¼Œå› ä¸ºtopicsè¡¨æ²¡æœ‰user_idå­—æ®µ
        cursor.execute("""
            SELECT COUNT(DISTINCT t.owner_user_id)
            FROM talks t
            JOIN topics tp ON t.topic_id = tp.topic_id
            WHERE tp.group_id = ?
        """, (group_id,))
        users_count = cursor.fetchone()[0]

        # è·å–æœ€æ–°è¯é¢˜æ—¶é—´
        cursor.execute("SELECT MAX(create_time) FROM topics WHERE group_id = ?", (group_id,))
        latest_topic_time = cursor.fetchone()[0]

        # è·å–æœ€æ—©è¯é¢˜æ—¶é—´
        cursor.execute("SELECT MIN(create_time) FROM topics WHERE group_id = ?", (group_id,))
        earliest_topic_time = cursor.fetchone()[0]

        # è·å–æ€»ç‚¹èµæ•°
        cursor.execute("SELECT SUM(likes_count) FROM topics WHERE group_id = ?", (group_id,))
        total_likes = cursor.fetchone()[0] or 0

        # è·å–æ€»è¯„è®ºæ•°
        cursor.execute("SELECT SUM(comments_count) FROM topics WHERE group_id = ?", (group_id,))
        total_comments = cursor.fetchone()[0] or 0

        # è·å–æ€»é˜…è¯»æ•°
        cursor.execute("SELECT SUM(reading_count) FROM topics WHERE group_id = ?", (group_id,))
        total_readings = cursor.fetchone()[0] or 0

        return {
            "group_id": group_id,
            "topics_count": topics_count,
            "users_count": users_count,
            "latest_topic_time": latest_topic_time,
            "earliest_topic_time": earliest_topic_time,
            "total_likes": total_likes,
            "total_comments": total_comments,
            "total_readings": total_readings
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"è·å–ç¾¤ç»„ç»Ÿè®¡å¤±è´¥: {str(e)}")

@app.get("/api/groups/{group_id}/database-info")
async def get_group_database_info(group_id: int):
    """è·å–æŒ‡å®šç¾¤ç»„çš„æ•°æ®åº“ä¿¡æ¯"""
    try:
        path_manager = get_db_path_manager()
        db_info = path_manager.get_database_info(str(group_id))

        return {
            "group_id": group_id,
            "database_info": db_info
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"è·å–æ•°æ®åº“ä¿¡æ¯å¤±è´¥: {str(e)}")

@app.delete("/api/groups/{group_id}/topics")
async def delete_group_topics(group_id: int):
    """åˆ é™¤æŒ‡å®šç¾¤ç»„çš„æ‰€æœ‰è¯é¢˜æ•°æ®"""
    try:
        # ä½¿ç”¨æŒ‡å®šç¾¤ç»„çš„çˆ¬è™«å®ä¾‹
        crawler = get_crawler_for_group(str(group_id))

        # è·å–åˆ é™¤å‰çš„ç»Ÿè®¡ä¿¡æ¯
        crawler.db.cursor.execute('SELECT COUNT(*) FROM topics WHERE group_id = ?', (group_id,))
        topics_count = crawler.db.cursor.fetchone()[0]

        if topics_count == 0:
            return {
                "message": "è¯¥ç¾¤ç»„æ²¡æœ‰è¯é¢˜æ•°æ®",
                "deleted_count": 0
            }

        # åˆ é™¤ç›¸å…³æ•°æ®ï¼ˆæŒ‰ç…§å¤–é”®ä¾èµ–é¡ºåºï¼‰
        tables_to_clean = [
            ('user_liked_emojis', 'topic_id'),
            ('like_emojis', 'topic_id'),
            ('likes', 'topic_id'),
            ('images', 'topic_id'),
            ('comments', 'topic_id'),
            ('answers', 'topic_id'),
            ('questions', 'topic_id'),
            ('articles', 'topic_id'),
            ('talks', 'topic_id'),
            ('topic_files', 'topic_id'),  # æ·»åŠ è¯é¢˜æ–‡ä»¶è¡¨
            ('topic_tags', 'topic_id'),   # æ·»åŠ è¯é¢˜æ ‡ç­¾å…³è”è¡¨
            ('topics', 'group_id')
        ]

        deleted_counts = {}

        for table, id_column in tables_to_clean:
            if id_column == 'group_id':
                # ç›´æ¥æŒ‰group_idåˆ é™¤
                crawler.db.cursor.execute(f'DELETE FROM {table} WHERE {id_column} = ?', (group_id,))
            else:
                # æŒ‰topic_idåˆ é™¤ï¼Œéœ€è¦å…ˆæ‰¾åˆ°è¯¥ç¾¤ç»„çš„æ‰€æœ‰topic_id
                crawler.db.cursor.execute(f'''
                    DELETE FROM {table}
                    WHERE {id_column} IN (
                        SELECT topic_id FROM topics WHERE group_id = ?
                    )
                ''', (group_id,))

            deleted_counts[table] = crawler.db.cursor.rowcount

        # æäº¤äº‹åŠ¡
        crawler.db.conn.commit()

        return {
            "message": f"æˆåŠŸåˆ é™¤ç¾¤ç»„ {group_id} çš„æ‰€æœ‰è¯é¢˜æ•°æ®",
            "deleted_topics_count": topics_count,
            "deleted_details": deleted_counts
        }

    except Exception as e:
        # å›æ»šäº‹åŠ¡
        crawler.db.conn.rollback()
        raise HTTPException(status_code=500, detail=f"åˆ é™¤è¯é¢˜æ•°æ®å¤±è´¥: {str(e)}")

@app.get("/api/tasks/{task_id}/logs")
async def get_task_logs(task_id: str):
    """è·å–ä»»åŠ¡æ—¥å¿—"""
    if task_id not in task_logs:
        raise HTTPException(status_code=404, detail="ä»»åŠ¡ä¸å­˜åœ¨")

    return {
        "task_id": task_id,
        "logs": task_logs[task_id]
    }

@app.get("/api/tasks/{task_id}/stream")
async def stream_task_logs(task_id: str):
    """SSEæµå¼ä¼ è¾“ä»»åŠ¡æ—¥å¿—"""
    async def event_stream():
        # åˆå§‹åŒ–è¿æ¥
        if task_id not in sse_connections:
            sse_connections[task_id] = []

        # å‘é€å†å²æ—¥å¿—
        if task_id in task_logs:
            for log in task_logs[task_id]:
                yield f"data: {json.dumps({'type': 'log', 'message': log})}\n\n"

        # å‘é€ä»»åŠ¡çŠ¶æ€
        if task_id in current_tasks:
            task = current_tasks[task_id]
            yield f"data: {json.dumps({'type': 'status', 'status': task['status'], 'message': task['message']})}\n\n"

        # è®°å½•å½“å‰æ—¥å¿—æ•°é‡ï¼Œç”¨äºæ£€æµ‹æ–°æ—¥å¿—
        last_log_count = len(task_logs.get(task_id, []))

        # ä¿æŒè¿æ¥æ´»è·ƒ
        try:
            while True:
                # æ£€æŸ¥æ˜¯å¦æœ‰æ–°æ—¥å¿—
                current_log_count = len(task_logs.get(task_id, []))
                if current_log_count > last_log_count:
                    # å‘é€æ–°æ—¥å¿—
                    new_logs = task_logs[task_id][last_log_count:]
                    for log in new_logs:
                        yield f"data: {json.dumps({'type': 'log', 'message': log})}\n\n"
                    last_log_count = current_log_count

                # æ£€æŸ¥ä»»åŠ¡çŠ¶æ€å˜åŒ–
                if task_id in current_tasks:
                    task = current_tasks[task_id]
                    yield f"data: {json.dumps({'type': 'status', 'status': task['status'], 'message': task['message']})}\n\n"

                    if task['status'] in ['completed', 'failed', 'cancelled']:
                        break

                # å‘é€å¿ƒè·³
                yield f"data: {json.dumps({'type': 'heartbeat'})}\n\n"
                await asyncio.sleep(0.5)  # æ›´é¢‘ç¹çš„æ£€æŸ¥

        except asyncio.CancelledError:
            # å®¢æˆ·ç«¯æ–­å¼€è¿æ¥
            pass

    return StreamingResponse(
        event_stream(),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
            "Access-Control-Allow-Origin": "*",
            "Access-Control-Allow-Headers": "*",
        }
    )

# å›¾ç‰‡ä»£ç†API
@app.get("/api/proxy/image")
async def proxy_image(url: str):
    """å›¾ç‰‡ä»£ç†ï¼Œè§£å†³ç›—é“¾é—®é¢˜"""
    import requests
    from fastapi.responses import Response

    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Referer': 'https://wx.zsxq.com/',
            'Accept': 'image/webp,image/apng,image/*,*/*;q=0.8',
        }

        response = requests.get(url, headers=headers, timeout=10)
        response.raise_for_status()

        return Response(
            content=response.content,
            media_type=response.headers.get('content-type', 'image/jpeg'),
            headers={
                'Cache-Control': 'public, max-age=3600',
                'Access-Control-Allow-Origin': '*'
            }
        )
    except Exception as e:
        raise HTTPException(status_code=404, detail=f"å›¾ç‰‡åŠ è½½å¤±è´¥: {str(e)}")

# è®¾ç½®ç›¸å…³APIè·¯ç”±
@app.get("/api/settings/crawler")
async def get_crawler_settings():
    """è·å–çˆ¬è™«è®¾ç½®"""
    try:
        crawler = get_crawler_safe()
        if not crawler:
            return {
                "min_delay": 2.0,
                "max_delay": 5.0,
                "long_delay_interval": 15,
                "timestamp_offset_ms": 1,
                "debug_mode": False
            }

        return {
            "min_delay": crawler.min_delay,
            "max_delay": crawler.max_delay,
            "long_delay_interval": crawler.long_delay_interval,
            "timestamp_offset_ms": crawler.timestamp_offset_ms,
            "debug_mode": crawler.debug_mode
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"è·å–çˆ¬è™«è®¾ç½®å¤±è´¥: {str(e)}")

class CrawlerSettingsRequest(BaseModel):
    min_delay: float = Field(default=2.0, ge=0.5, le=10.0)
    max_delay: float = Field(default=5.0, ge=1.0, le=20.0)
    long_delay_interval: int = Field(default=15, ge=5, le=100)
    timestamp_offset_ms: int = Field(default=1, ge=0, le=1000)
    debug_mode: bool = Field(default=False)

@app.post("/api/settings/crawler")
async def update_crawler_settings(request: CrawlerSettingsRequest):
    """æ›´æ–°çˆ¬è™«è®¾ç½®"""
    try:
        crawler = get_crawler_safe()
        if not crawler:
            raise HTTPException(status_code=404, detail="çˆ¬è™«æœªåˆå§‹åŒ–")

        # éªŒè¯è®¾ç½®
        if request.min_delay >= request.max_delay:
            raise HTTPException(status_code=400, detail="æœ€å°å»¶è¿Ÿå¿…é¡»å°äºæœ€å¤§å»¶è¿Ÿ")

        # æ›´æ–°è®¾ç½®
        crawler.min_delay = request.min_delay
        crawler.max_delay = request.max_delay
        crawler.long_delay_interval = request.long_delay_interval
        crawler.timestamp_offset_ms = request.timestamp_offset_ms
        crawler.debug_mode = request.debug_mode

        return {
            "message": "çˆ¬è™«è®¾ç½®å·²æ›´æ–°",
            "settings": {
                "min_delay": crawler.min_delay,
                "max_delay": crawler.max_delay,
                "long_delay_interval": crawler.long_delay_interval,
                "timestamp_offset_ms": crawler.timestamp_offset_ms,
                "debug_mode": crawler.debug_mode
            }
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"æ›´æ–°çˆ¬è™«è®¾ç½®å¤±è´¥: {str(e)}")

@app.get("/api/settings/downloader")
async def get_downloader_settings():
    """è·å–æ–‡ä»¶ä¸‹è½½å™¨è®¾ç½®"""
    try:
        crawler = get_crawler_safe()
        if not crawler:
            return {
                "download_interval_min": 30,
                "download_interval_max": 60,
                "long_delay_interval": 10,
                "long_delay_min": 300,
                "long_delay_max": 600
            }

        downloader = crawler.get_file_downloader()
        return {
            "download_interval_min": downloader.download_interval_min,
            "download_interval_max": downloader.download_interval_max,
            "long_delay_interval": downloader.long_delay_interval,
            "long_delay_min": downloader.long_delay_min,
            "long_delay_max": downloader.long_delay_max
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"è·å–ä¸‹è½½å™¨è®¾ç½®å¤±è´¥: {str(e)}")

class DownloaderSettingsRequest(BaseModel):
    download_interval_min: int = Field(default=30, ge=1, le=300)
    download_interval_max: int = Field(default=60, ge=5, le=600)
    long_delay_interval: int = Field(default=10, ge=1, le=100)
    long_delay_min: int = Field(default=300, ge=60, le=1800)
    long_delay_max: int = Field(default=600, ge=120, le=3600)

@app.post("/api/settings/downloader")
async def update_downloader_settings(request: DownloaderSettingsRequest):
    """æ›´æ–°æ–‡ä»¶ä¸‹è½½å™¨è®¾ç½®"""
    try:
        crawler = get_crawler_safe()
        if not crawler:
            raise HTTPException(status_code=404, detail="çˆ¬è™«æœªåˆå§‹åŒ–")

        # éªŒè¯è®¾ç½®
        if request.download_interval_min >= request.download_interval_max:
            raise HTTPException(status_code=400, detail="æœ€å°ä¸‹è½½é—´éš”å¿…é¡»å°äºæœ€å¤§ä¸‹è½½é—´éš”")

        if request.long_delay_min >= request.long_delay_max:
            raise HTTPException(status_code=400, detail="æœ€å°é•¿ä¼‘çœ æ—¶é—´å¿…é¡»å°äºæœ€å¤§é•¿ä¼‘çœ æ—¶é—´")

        downloader = crawler.get_file_downloader()

        # æ›´æ–°è®¾ç½®
        downloader.download_interval_min = request.download_interval_min
        downloader.download_interval_max = request.download_interval_max
        downloader.long_delay_interval = request.long_delay_interval
        downloader.long_delay_min = request.long_delay_min
        downloader.long_delay_max = request.long_delay_max

        return {
            "message": "ä¸‹è½½å™¨è®¾ç½®å·²æ›´æ–°",
            "settings": {
                "download_interval_min": downloader.download_interval_min,
                "download_interval_max": downloader.download_interval_max,
                "long_delay_interval": downloader.long_delay_interval,
                "long_delay_min": downloader.long_delay_min,
                "long_delay_max": downloader.long_delay_max
            }
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"æ›´æ–°ä¸‹è½½å™¨è®¾ç½®å¤±è´¥: {str(e)}")

# =========================
# è‡ªåŠ¨è´¦å·åŒ¹é…ç¼“å­˜ä¸è¾…åŠ©å‡½æ•°
# =========================
ACCOUNT_DETECT_TTL_SECONDS = 300
_account_detect_cache: Dict[str, Any] = {
    "built_at": 0,
    "group_to_account": {},
    "cookie_by_account": {}
}

def _get_all_account_sources() -> List[Dict[str, Any]]:
    """ç»„åˆè´¦å·æ¥æºï¼šaccounts.json + config.tomlé»˜è®¤è´¦å·"""
    sources: List[Dict[str, Any]] = []
    try:
        # è´¦å·ç®¡ç†ä¸­çš„è´¦å·ï¼ˆå«cookieï¼‰
        accounts = am_get_accounts(mask_cookie=False)
        if accounts:
            sources.extend(accounts)
    except Exception:
        pass
    # è¿½åŠ  config.toml çš„é»˜è®¤cookieä½œä¸ºä¼ªè´¦å·
    try:
        cfg = load_config()
        auth = cfg.get('auth', {}) if cfg else {}
        default_cookie = auth.get('cookie', '')
        if default_cookie and default_cookie != "your_cookie_here":
            sources.append({
                "id": "default",
                "name": "é»˜è®¤è´¦å·",
                "cookie": default_cookie,
                "is_default": True,
                "created_at": None
            })
    except Exception:
        pass
    return sources

def build_account_group_detection(force_refresh: bool = False) -> Dict[str, Dict[str, Any]]:
    """
    æ„å»ºè‡ªåŠ¨åŒ¹é…æ˜ å°„ï¼šgroup_id -> è´¦å·æ‘˜è¦
    éå†æ‰€æœ‰è´¦å·æ¥æºï¼Œè°ƒç”¨å®˜æ–¹ /v2/groups è·å–å…¶å¯è®¿é—®ç¾¤ç»„è¿›è¡Œæ¯”å¯¹ã€‚
    ä½¿ç”¨å†…å­˜ç¼“å­˜å‡å°‘é¢‘ç¹è¯·æ±‚ã€‚
    """
    now = time.time()
    cache = _account_detect_cache
    if (not force_refresh
        and cache.get("group_to_account")
        and now - cache.get("built_at", 0) < ACCOUNT_DETECT_TTL_SECONDS):
        return cache["group_to_account"]

    group_to_account: Dict[str, Dict[str, Any]] = {}
    cookie_by_account: Dict[str, str] = {}

    sources = _get_all_account_sources()
    for src in sources:
        cookie = src.get("cookie", "")
        acc_id = src.get("id", "default")
        if not cookie or cookie == "your_cookie_here":
            continue

        # è®°å½•è´¦å·å¯¹åº”cookie
        cookie_by_account[acc_id] = cookie

        try:
            groups = fetch_groups_from_api(cookie)
            for g in groups or []:
                gid = str(g.get("group_id"))
                if gid and gid not in group_to_account:
                    group_to_account[gid] = {
                        "id": acc_id,
                        "name": src.get("name") or ("é»˜è®¤è´¦å·" if acc_id == "default" else acc_id),
                        "is_default": bool(src.get("is_default") or acc_id == "default"),
                        "created_at": src.get("created_at"),
                        "cookie": "***"
                    }
        except Exception:
            # å¿½ç•¥å•ä¸ªè´¦å·å¤±è´¥
            continue

    cache["group_to_account"] = group_to_account
    cache["cookie_by_account"] = cookie_by_account
    cache["built_at"] = now
    return group_to_account

def get_cookie_for_group(group_id: str) -> str:
    """æ ¹æ®è‡ªåŠ¨åŒ¹é…ç»“æœé€‰æ‹©ç”¨äºè¯¥ç¾¤ç»„çš„Cookieï¼Œå¤±è´¥åˆ™å›é€€åˆ°config.toml"""
    mapping = build_account_group_detection(force_refresh=False)
    summary = mapping.get(str(group_id))
    cookie = None
    if summary:
        cookie = _account_detect_cache.get("cookie_by_account", {}).get(summary["id"])
    if not cookie:
        cfg = load_config()
        auth = cfg.get('auth', {}) if cfg else {}
        cookie = auth.get('cookie', '')
    return cookie

def get_account_summary_for_group_auto(group_id: str) -> Optional[Dict[str, Any]]:
    """è¿”å›è‡ªåŠ¨åŒ¹é…åˆ°çš„è´¦å·æ‘˜è¦ï¼›è‹¥æ— å‘½ä¸­ä¸”å­˜åœ¨é»˜è®¤cookieï¼Œåˆ™è¿”å›é»˜è®¤å ä½æ‘˜è¦"""
    mapping = build_account_group_detection(force_refresh=False)
    summary = mapping.get(str(group_id))
    if summary:
        return summary
    cfg = load_config()
    auth = cfg.get('auth', {}) if cfg else {}
    default_cookie = auth.get('cookie', '')
    if default_cookie:
        return {
            "id": "default",
            "name": "é»˜è®¤è´¦å·",
            "is_default": True,
            "created_at": None,
            "cookie": "***"
        }
    return None

# =========================
# æ–°å¢ï¼šæŒ‰æ—¶é—´åŒºé—´çˆ¬å–
# =========================

class CrawlTimeRangeRequest(BaseModel):
    startTime: Optional[str] = Field(default=None, description="å¼€å§‹æ—¶é—´ï¼Œæ”¯æŒ YYYY-MM-DD æˆ– ISO8601ï¼Œç¼ºçœåˆ™æŒ‰ lastDays æ¨å¯¼")
    endTime: Optional[str] = Field(default=None, description="ç»“æŸæ—¶é—´ï¼Œé»˜è®¤å½“å‰æ—¶é—´ï¼ˆæœ¬åœ°ä¸œå…«åŒºï¼‰")
    lastDays: Optional[int] = Field(default=None, ge=1, le=3650, description="æœ€è¿‘Nå¤©ï¼ˆä¸ startTime/endTime äº’æ–¥ä¼˜å…ˆï¼›å½“ startTime ç¼ºçœæ—¶å¯ç”¨ï¼‰")
    perPage: Optional[int] = Field(default=20, ge=1, le=100, description="æ¯é¡µæ•°é‡")
    # å¯é€‰çš„éšæœºé—´éš”è®¾ç½®ï¼ˆä¸å…¶ä»–çˆ¬å–æ¥å£ä¿æŒä¸€è‡´ï¼‰
    crawlIntervalMin: Optional[float] = Field(default=None, ge=1.0, le=60.0, description="çˆ¬å–é—´éš”æœ€å°å€¼(ç§’)")
    crawlIntervalMax: Optional[float] = Field(default=None, ge=1.0, le=60.0, description="çˆ¬å–é—´éš”æœ€å¤§å€¼(ç§’)")
    longSleepIntervalMin: Optional[float] = Field(default=None, ge=60.0, le=3600.0, description="é•¿ä¼‘çœ é—´éš”æœ€å°å€¼(ç§’)")
    longSleepIntervalMax: Optional[float] = Field(default=None, ge=60.0, le=3600.0, description="é•¿ä¼‘çœ é—´éš”æœ€å¤§å€¼(ç§’)")
    pagesPerBatch: Optional[int] = Field(default=None, ge=5, le=50, description="æ¯æ‰¹æ¬¡é¡µé¢æ•°")


def run_crawl_time_range_task(task_id: str, group_id: str, request: "CrawlTimeRangeRequest"):
    """åå°æ‰§è¡Œâ€œæŒ‰æ—¶é—´åŒºé—´çˆ¬å–â€ä»»åŠ¡ï¼šä»…å¯¼å…¥ä½äºåŒºé—´ [startTime, endTime] å†…çš„è¯é¢˜"""
    try:
        from datetime import datetime, timedelta, timezone

        # è§£æç”¨æˆ·è¾“å…¥æ—¶é—´
        def parse_user_time(s: Optional[str]) -> Optional[datetime]:
            if not s:
                return None
            t = s.strip()
            try:
                # ä»…æ—¥æœŸï¼šYYYY-MM-DD -> å½“å¤©00:00:00ï¼ˆä¸œå…«åŒºï¼‰
                if len(t) == 10 and t[4] == '-' and t[7] == '-':
                    dt = datetime.strptime(t, '%Y-%m-%d')
                    return dt.replace(tzinfo=timezone(timedelta(hours=8)))
                # datetime-local (æ— ç§’)ï¼šYYYY-MM-DDTHH:MM
                if 'T' in t and len(t) == 16:
                    t = t + ':00'
                # å°¾éƒ¨Z -> +00:00
                if t.endswith('Z'):
                    t = t.replace('Z', '+00:00')
                # å…¼å®¹ +0800 -> +08:00
                if len(t) >= 24 and (t[-5] in ['+', '-']) and t[-3] != ':':
                    t = t[:-2] + ':' + t[-2:]
                dt = datetime.fromisoformat(t)
                if dt.tzinfo is None:
                    dt = dt.replace(tzinfo=timezone(timedelta(hours=8)))
                return dt
            except Exception:
                return None

        bj_tz = timezone(timedelta(hours=8))
        now_bj = datetime.now(bj_tz)

        start_dt = parse_user_time(request.startTime)
        end_dt = parse_user_time(request.endTime) if request.endTime else None

        # è‹¥æŒ‡å®šäº†æœ€è¿‘Nå¤©ï¼Œä»¥ end_dtï¼ˆé»˜è®¤ç°åœ¨ï¼‰ä¸ºç»ˆç‚¹æ¨å¯¼ start_dt
        if request.lastDays and request.lastDays > 0:
            if end_dt is None:
                end_dt = now_bj
            start_dt = end_dt - timedelta(days=request.lastDays)

        # é»˜è®¤ end_dt = ç°åœ¨
        if end_dt is None:
            end_dt = now_bj
        # é»˜è®¤ start_dt = end_dt - 30å¤©
        if start_dt is None:
            start_dt = end_dt - timedelta(days=30)

        # ä¿è¯æ—¶é—´é¡ºåº
        if start_dt > end_dt:
            start_dt, end_dt = end_dt, start_dt

        update_task(task_id, "running", "å¼€å§‹æŒ‰æ—¶é—´åŒºé—´çˆ¬å–...")
        add_task_log(task_id, f"ğŸ—“ï¸ æ—¶é—´èŒƒå›´: {start_dt.isoformat()} ~ {end_dt.isoformat()}")

        # åœæ­¢æ£€æŸ¥
        def stop_check():
            return is_task_stopped(task_id)

        # çˆ¬è™«å®ä¾‹ï¼ˆç»‘å®šè¯¥ç¾¤ç»„ï¼‰
        def log_callback(message: str):
            add_task_log(task_id, message)

        cookie = get_cookie_for_group(group_id)
        path_manager = get_db_path_manager()
        db_path = path_manager.get_topics_db_path(group_id)

        crawler = ZSXQInteractiveCrawler(cookie, group_id, db_path, log_callback)
        crawler.stop_check_func = stop_check

        # å¯é€‰ï¼šåº”ç”¨è‡ªå®šä¹‰é—´éš”è®¾ç½®
        if any([
            request.crawlIntervalMin, request.crawlIntervalMax,
            request.longSleepIntervalMin, request.longSleepIntervalMax,
            request.pagesPerBatch
        ]):
            crawler.set_custom_intervals(
                crawl_interval_min=request.crawlIntervalMin,
                crawl_interval_max=request.crawlIntervalMax,
                long_sleep_interval_min=request.longSleepIntervalMin,
                long_sleep_interval_max=request.longSleepIntervalMax,
                pages_per_batch=request.pagesPerBatch
            )

        per_page = request.perPage or 20
        total_stats = {'new_topics': 0, 'updated_topics': 0, 'errors': 0, 'pages': 0}
        end_time_param = None  # ä»æœ€æ–°å¼€å§‹
        max_retries_per_page = 10

        while True:
            if is_task_stopped(task_id):
                add_task_log(task_id, "ğŸ›‘ ä»»åŠ¡å·²åœæ­¢")
                break

            retry = 0
            page_processed = False
            last_time_dt_in_page = None

            while retry < max_retries_per_page:
                if is_task_stopped(task_id):
                    break

                data = crawler.fetch_topics_safe(
                    scope="all",
                    count=per_page,
                    end_time=end_time_param,
                    is_historical=True if end_time_param else False
                )

                # ä¼šå‘˜è¿‡æœŸ
                if data and isinstance(data, dict) and data.get('expired'):
                    add_task_log(task_id, f"âŒ ä¼šå‘˜å·²è¿‡æœŸ: {data.get('message')}")
                    update_task(task_id, "failed", "ä¼šå‘˜å·²è¿‡æœŸ", data)
                    return

                if not data:
                    retry += 1
                    total_stats['errors'] += 1
                    add_task_log(task_id, f"âŒ é¡µé¢è·å–å¤±è´¥ (é‡è¯•{retry}/{max_retries_per_page})")
                    continue

                topics = (data.get('resp_data', {}) or {}).get('topics', []) or []
                if not topics:
                    add_task_log(task_id, "ğŸ“­ æ— æ›´å¤šæ•°æ®ï¼Œä»»åŠ¡ç»“æŸ")
                    page_processed = True
                    break

                # è¿‡æ»¤æ—¶é—´èŒƒå›´
                from datetime import datetime
                filtered = []
                for t in topics:
                    ts = t.get('create_time')
                    dt = None
                    try:
                        if ts:
                            ts_fixed = ts.replace('+0800', '+08:00') if ts.endswith('+0800') else ts
                            dt = datetime.fromisoformat(ts_fixed)
                    except Exception:
                        dt = None

                    if dt:
                        last_time_dt_in_page = dt  # è¯¥é¡µæ•°æ®æŒ‰æ—¶é—´é™åºï¼›å¾ªç¯ç»“æŸåæŒæœ‰æœ€åï¼ˆæœ€è€ï¼‰æ—¶é—´
                        if start_dt <= dt <= end_dt:
                            filtered.append(t)

                # ä»…å¯¼å…¥æ—¶é—´èŒƒå›´å†…çš„æ•°æ®
                if filtered:
                    filtered_data = {'succeeded': True, 'resp_data': {'topics': filtered}}
                    page_stats = crawler.store_batch_data(filtered_data)
                    total_stats['new_topics'] += page_stats.get('new_topics', 0)
                    total_stats['updated_topics'] += page_stats.get('updated_topics', 0)
                    total_stats['errors'] += page_stats.get('errors', 0)

                total_stats['pages'] += 1
                page_processed = True

                # è®¡ç®—ä¸‹ä¸€é¡µçš„ end_timeï¼ˆä½¿ç”¨è¯¥é¡µæœ€è€è¯é¢˜æ—¶é—´ - åç§»æ¯«ç§’ï¼‰
                oldest_in_page = topics[-1].get('create_time')
                try:
                    dt_oldest = datetime.fromisoformat(oldest_in_page.replace('+0800', '+08:00'))
                    dt_oldest = dt_oldest - timedelta(milliseconds=crawler.timestamp_offset_ms)
                    end_time_param = dt_oldest.strftime('%Y-%m-%dT%H:%M:%S.%f')[:-3] + '+0800'
                except Exception:
                    end_time_param = oldest_in_page

                # è‹¥è¯¥é¡µæœ€è€æ—¶é—´å·²æ—©äº start_dtï¼Œåˆ™åç»­æ›´è€æ•°æ®å‡ä¸åœ¨èŒƒå›´å†…ï¼Œç»“æŸ
                if last_time_dt_in_page and last_time_dt_in_page < start_dt:
                    add_task_log(task_id, "âœ… å·²åˆ°è¾¾èµ·å§‹æ—¶é—´ä¹‹å‰ï¼Œä»»åŠ¡ç»“æŸ")
                    break

                # æˆåŠŸå¤„ç†åè¿›è¡Œé•¿ä¼‘çœ æ£€æŸ¥
                crawler.check_page_long_delay()
                break  # æˆåŠŸåè·³å‡ºé‡è¯•å¾ªç¯

            if not page_processed:
                add_task_log(task_id, "ğŸš« å½“å‰é¡µé¢è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°ï¼Œç»ˆæ­¢ä»»åŠ¡")
                break

            # ç»“æŸæ¡ä»¶ï¼šæ²¡æœ‰ä¸‹ä¸€é¡µæ—¶é—´æˆ–å·²è¶Šè¿‡èµ·å§‹è¾¹ç•Œ
            if not end_time_param or (last_time_dt_in_page and last_time_dt_in_page < start_dt):
                break

        update_task(task_id, "completed", "æ—¶é—´åŒºé—´çˆ¬å–å®Œæˆ", total_stats)
    except Exception as e:
        if not is_task_stopped(task_id):
            add_task_log(task_id, f"âŒ æ—¶é—´åŒºé—´çˆ¬å–å¤±è´¥: {str(e)}")
            update_task(task_id, "failed", f"æ—¶é—´åŒºé—´çˆ¬å–å¤±è´¥: {str(e)}")


@app.post("/api/crawl/range/{group_id}")
async def crawl_by_time_range(group_id: str, request: CrawlTimeRangeRequest, background_tasks: BackgroundTasks):
    """æŒ‰æ—¶é—´åŒºé—´çˆ¬å–è¯é¢˜ï¼ˆæ”¯æŒæœ€è¿‘Nå¤©æˆ–è‡ªå®šä¹‰å¼€å§‹/ç»“æŸæ—¶é—´ï¼‰"""
    try:
        task_id = create_task("crawl_time_range", f"æŒ‰æ—¶é—´åŒºé—´çˆ¬å– (ç¾¤ç»„: {group_id})")
        background_tasks.add_task(run_crawl_time_range_task, task_id, group_id, request)
        return {"task_id": task_id, "message": "ä»»åŠ¡å·²åˆ›å»ºï¼Œæ­£åœ¨åå°æ‰§è¡Œ"}
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"åˆ›å»ºæ—¶é—´åŒºé—´çˆ¬å–ä»»åŠ¡å¤±è´¥: {str(e)}")
@app.delete("/api/groups/{group_id}")
async def delete_group_local(group_id: str):
    """
    åˆ é™¤æŒ‡å®šç¤¾ç¾¤çš„æœ¬åœ°æ•°æ®ï¼ˆæ•°æ®åº“ã€ä¸‹è½½æ–‡ä»¶ã€å›¾ç‰‡ç¼“å­˜ï¼‰ï¼Œä¸å½±å“è´¦å·å¯¹è¯¥ç¤¾ç¾¤çš„è®¿é—®æƒé™
    """
    try:
        details = {
            "topics_db_removed": False,
            "files_db_removed": False,
            "downloads_dir_removed": False,
            "images_cache_removed": False,
            "group_dir_removed": False,
        }

        # å°è¯•å…³é—­æ•°æ®åº“è¿æ¥ï¼Œé¿å…æ–‡ä»¶å ç”¨
        try:
            crawler = get_crawler_for_group(group_id)
            try:
                if hasattr(crawler, "file_downloader") and crawler.file_downloader:
                    if hasattr(crawler.file_downloader, "file_db") and crawler.file_downloader.file_db:
                        crawler.file_downloader.file_db.close()
                        print(f"âœ… å·²å…³é—­æ–‡ä»¶æ•°æ®åº“è¿æ¥ï¼ˆç¾¤ {group_id}ï¼‰")
            except Exception as e:
                print(f"âš ï¸ å…³é—­æ–‡ä»¶æ•°æ®åº“è¿æ¥æ—¶å‡ºé”™: {e}")
            try:
                if hasattr(crawler, "db") and crawler.db:
                    crawler.db.close()
                    print(f"âœ… å·²å…³é—­è¯é¢˜æ•°æ®åº“è¿æ¥ï¼ˆç¾¤ {group_id}ï¼‰")
            except Exception as e:
                print(f"âš ï¸ å…³é—­è¯é¢˜æ•°æ®åº“è¿æ¥æ—¶å‡ºé”™: {e}")
        except Exception as e:
            print(f"âš ï¸ è·å–çˆ¬è™«å®ä¾‹ä»¥å…³é—­è¿æ¥å¤±è´¥: {e}")

        # åƒåœ¾å›æ”¶ + ç­‰å¾…ç‰‡åˆ»ï¼Œç¡®ä¿å¥æŸ„é‡Šæ”¾
        import gc, time, shutil
        gc.collect()
        time.sleep(0.3)

        path_manager = get_db_path_manager()
        group_dir = path_manager.get_group_dir(group_id)
        topics_db = path_manager.get_topics_db_path(group_id)
        files_db = path_manager.get_files_db_path(group_id)

        # åˆ é™¤è¯é¢˜æ•°æ®åº“
        try:
            if os.path.exists(topics_db):
                os.remove(topics_db)
                details["topics_db_removed"] = True
                print(f"ğŸ—‘ï¸ å·²åˆ é™¤è¯é¢˜æ•°æ®åº“: {topics_db}")
        except PermissionError as pe:
            raise HTTPException(status_code=500, detail=f"è¯é¢˜æ•°æ®åº“è¢«å ç”¨ï¼Œæ— æ³•åˆ é™¤: {pe}")
        except Exception as e:
            print(f"âš ï¸ åˆ é™¤è¯é¢˜æ•°æ®åº“å¤±è´¥: {e}")

        # åˆ é™¤æ–‡ä»¶æ•°æ®åº“
        try:
            if os.path.exists(files_db):
                os.remove(files_db)
                details["files_db_removed"] = True
                print(f"ğŸ—‘ï¸ å·²åˆ é™¤æ–‡ä»¶æ•°æ®åº“: {files_db}")
        except PermissionError as pe:
            raise HTTPException(status_code=500, detail=f"æ–‡ä»¶æ•°æ®åº“è¢«å ç”¨ï¼Œæ— æ³•åˆ é™¤: {pe}")
        except Exception as e:
            print(f"âš ï¸ åˆ é™¤æ–‡ä»¶æ•°æ®åº“å¤±è´¥: {e}")

        # åˆ é™¤ä¸‹è½½ç›®å½•
        downloads_dir = os.path.join(group_dir, "downloads")
        if os.path.exists(downloads_dir):
            try:
                shutil.rmtree(downloads_dir, ignore_errors=False)
                details["downloads_dir_removed"] = True
                print(f"ğŸ—‘ï¸ å·²åˆ é™¤ä¸‹è½½ç›®å½•: {downloads_dir}")
            except Exception as e:
                print(f"âš ï¸ åˆ é™¤ä¸‹è½½ç›®å½•å¤±è´¥: {e}")

        # æ¸…ç©ºå¹¶åˆ é™¤å›¾ç‰‡ç¼“å­˜ç›®å½•ï¼ŒåŒæ—¶é‡Šæ”¾ç¼“å­˜ç®¡ç†å™¨
        try:
            from image_cache_manager import get_image_cache_manager, clear_group_cache_manager
            cache_manager = get_image_cache_manager(group_id)
            ok, msg = cache_manager.clear_cache()
            if ok:
                details["images_cache_removed"] = True
                print(f"ğŸ—‘ï¸ å›¾ç‰‡ç¼“å­˜æ¸…ç©º: {msg}")
            images_dir = os.path.join(group_dir, "images")
            if os.path.exists(images_dir):
                try:
                    shutil.rmtree(images_dir, ignore_errors=True)
                    print(f"ğŸ—‘ï¸ å·²åˆ é™¤å›¾ç‰‡ç¼“å­˜ç›®å½•: {images_dir}")
                except Exception as e:
                    print(f"âš ï¸ åˆ é™¤å›¾ç‰‡ç¼“å­˜ç›®å½•å¤±è´¥: {e}")
            clear_group_cache_manager(group_id)
        except Exception as e:
            print(f"âš ï¸ æ¸…ç†å›¾ç‰‡ç¼“å­˜å¤±è´¥: {e}")

        # è‹¥ç¾¤ç»„ç›®å½•å·²ç©ºï¼Œåˆ™åˆ é™¤è¯¥ç›®å½•
        try:
            if os.path.exists(group_dir) and len(os.listdir(group_dir)) == 0:
                os.rmdir(group_dir)
                details["group_dir_removed"] = True
                print(f"ğŸ—‘ï¸ å·²åˆ é™¤ç©ºç¾¤ç»„ç›®å½•: {group_dir}")
        except Exception as e:
            print(f"âš ï¸ åˆ é™¤ç¾¤ç»„ç›®å½•å¤±è´¥: {e}")

        # æ›´æ–°æœ¬åœ°ç¾¤ç¼“å­˜ï¼ˆä»ç¼“å­˜é›†åˆç§»é™¤ï¼‰
        try:
            gid_int = int(group_id)
            if gid_int in _local_groups_cache.get("ids", set()):
                _local_groups_cache["ids"].discard(gid_int)
                _local_groups_cache["scanned_at"] = time.time()
        except Exception as e:
            print(f"âš ï¸ æ›´æ–°æœ¬åœ°ç¾¤ç¼“å­˜å¤±è´¥: {e}")

        any_removed = any(details.values())
        return {
            "success": True,
            "message": f"ç¾¤ç»„ {group_id} æœ¬åœ°æ•°æ®" + ("å·²åˆ é™¤" if any_removed else "ä¸å­˜åœ¨"),
            "details": details,
        }
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"åˆ é™¤ç¾¤ç»„æœ¬åœ°æ•°æ®å¤±è´¥: {str(e)}")
if __name__ == "__main__":
    import sys
    port = 8001 if len(sys.argv) > 1 and sys.argv[1] == "--port" and len(sys.argv) > 2 else 8000
    if len(sys.argv) > 2 and sys.argv[1] == "--port":
        try:
            port = int(sys.argv[2])
        except ValueError:
            port = 8000
    uvicorn.run(app, host="0.0.0.0", port=port)
